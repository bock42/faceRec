{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# faceRec\n",
    "\n",
    "This IPython notebook will recognize a face in a video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "\n",
    "| Yale         | JAFFE        | FERG         | CK+          |\n",
    "| ------------ | ------------ | ------------ | ------------ |\n",
    "| 15 subjects  | 60 subjects  | 6  subjects  | 123 subjects |\n",
    "| 1 female     | 60 female    | 3  female    | 85 female    |\n",
    "| 1 image each | variable #   | variable #   | variable #   | \n",
    "| ------------ | ------------ | ------------ | ------------ |\n",
    "| happy        | happy        | happy        | happy        |\n",
    "| sad          | sad          | sad          | sad          |\n",
    "| surprise     | surprise     | surprise     | surprise     |\n",
    "| neutral      |              | neutral      | neutral      |\n",
    "|              | anger        | anger        | anger        |\n",
    "|              | disgust      | disgust      | disgust      |\n",
    "|              | fear         | fear         | fear         |\n",
    "\n",
    "\n",
    "CK+ Notes:\n",
    "\n",
    "4) The Emotion coded files (Emotion_labels.zip) - ONLY 327 of the 593 sequences have emotion sequences. This is because these are the only ones the fit the prototypic definition. Like the FACS files, there is only 1 Emotion file for each sequence which is the last frame (the peak frame). There should be only one entry and the number will range from 0-7 (i.e. 0=neutral, 1=anger, 2=contempt, 3=disgust, 4=fear, 5=happy, 6=sadness, 7=surprise). N.B there is only 327 files- IF THERE IS NO FILE IT MEANS THAT THERE IS NO EMOTION LABEL (sorry to be explicit but this will avoid confusion).\n",
    "\n",
    "\n",
    "JAFFE Notes:\n",
    "\n",
    "-------------- Semantic Ratings Data (Fear Excluded) --------------\n",
    "\n",
    "This rating experiment excluded fear images and and the fear adjective\n",
    "from the ratings. We did this because we thought that the expressors\n",
    "were not good at posing fear. There is some evidence in the scientific\n",
    "literature that fear may be processed differently from the other basic\n",
    "facial expressions.\n",
    "\n",
    "Please note that there are less subjects than the other experiment.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the faces and emotion files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CK+ images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from time import time\n",
    "\n",
    "# Initialize variables\n",
    "subjectNames = []\n",
    "subjectFiles = []\n",
    "subjectEmotions = []\n",
    "subjectImages = []\n",
    "emotions = ['anger','disgust','fear','joy','neutral','sadness','surprise']\n",
    "\n",
    "# OpenCV face classifier\n",
    "cascPath = '/usr/local/Cellar/opencv/2.4.13.2/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml'\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "# Face processing defaults\n",
    "outImgSize = 500\n",
    "scaleSize = 1.05\n",
    "numNeighbors = 3\n",
    "faceSize = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input images...\n",
      "done in 23.150s\n"
     ]
    }
   ],
   "source": [
    "# Set paths, variable values\n",
    "dbDir = '/Users/abock/Desktop/InsightProject/faceDatabases/CK+/'\n",
    "emoDir = '/Users/abock/Desktop/InsightProject/faceDatabases/CK+/Emotion/'\n",
    "emoNums = [1,3,4,5,0,6,7]\n",
    "\n",
    "print(\"Loading input images...\")\n",
    "t0 = time()\n",
    "\n",
    "# Find the images and emotion labels\n",
    "for subDir in glob.glob(emoDir + 'S*/'):\n",
    "    subject = os.path.basename(os.path.dirname(subDir))\n",
    "    # Append to subjectEmotions\n",
    "    subjectNames.append(subject)\n",
    "    for faceDir in glob.glob(subDir + '*/'):\n",
    "        dirNum = os.path.basename(os.path.dirname(faceDir))\n",
    "        for thisFile in glob.glob(faceDir + '*.txt'):\n",
    "            imDir = os.path.join(dbDir,subject,dirNum)\n",
    "            fName = os.path.basename(thisFile[:-12])\n",
    "            imFile = os.path.join(imDir,fName + '.png')\n",
    "            \n",
    "            with open(thisFile) as f:\n",
    "                for line in f:\n",
    "                    line = map(float, line.split())\n",
    "                    lineInt = int(line[0])\n",
    "#                     if lineInt != 2: # 2 is contempt\n",
    "                    if lineInt != 2 and lineInt !=4: # 2 is contempt, 4 is fear\n",
    "                        thisEmotion = emotions[emoNums.index(int(line[0]))]\n",
    "            \n",
    "            # Load the image\n",
    "            rawImg = cv2.imread(imFile)\n",
    "\n",
    "            # Detect faces in the image\n",
    "            faces = faceCascade.detectMultiScale(\n",
    "            rawImg,\n",
    "            scaleFactor=scaleSize,\n",
    "            minNeighbors=numNeighbors,\n",
    "            minSize=(faceSize, faceSize),\n",
    "            flags = cv2.CASCADE_SCALE_IMAGE\n",
    "            )\n",
    "\n",
    "            # If a face is detected\n",
    "            if len(faces):\n",
    "                # Convert to gray\n",
    "                gImg = cv2.cvtColor(rawImg, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # Resize just the face\n",
    "                x,y,w,h = faces[0,:]\n",
    "                faceImg = gImg[y:y+h,x:x+w]\n",
    "                img = cv2.resize(faceImg,(outImgSize,outImgSize),interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "                cv2.imshow('Image',img)\n",
    "                cv2.waitKey(30)\n",
    "\n",
    "                # Flatten\n",
    "                l = img.shape[0] * img.shape[1]\n",
    "                img = img.reshape(1, l)\n",
    "                \n",
    "                # Append to subjectNames\n",
    "                subjectNames.append(subject)\n",
    "                # Append to subjectEmotions\n",
    "                subjectEmotions.append(thisEmotion)\n",
    "                # Append to subjectFiles\n",
    "                subjectFiles.append(imFile) \n",
    "                # Append to output matrices\n",
    "                subjectImages.append(img[0])  \n",
    "                \n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FERG images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Set paths, variable values\n",
    "# dbDir = '/Users/abock/Desktop/InsightProject/faceDatabases/FERG/'\n",
    "\n",
    "# print(\"Loading input images...\")\n",
    "# t0 = time()\n",
    "\n",
    "# # Find the images and emotion labels\n",
    "# for subDir in glob.glob(dbDir + '*/'):\n",
    "#     subject = os.path.basename(os.path.dirname(subDir))\n",
    "    \n",
    "#     for faceDir in glob.glob(subDir + '*/'):\n",
    "#         tmp = os.path.basename(os.path.dirname(faceDir))\n",
    "#         thisEmotion = tmp[len(subject)+1:]\n",
    "#         for imFile in glob.glob(faceDir + '*.png'):\n",
    "#             # Load the image\n",
    "#             rawImg = cv2.imread(imFile)\n",
    "\n",
    "#             # Detect faces in the image\n",
    "#             faces = faceCascade.detectMultiScale(\n",
    "#             rawImg,\n",
    "#             scaleFactor=scaleSize,\n",
    "#             minNeighbors=numNeighbors,\n",
    "#             minSize=(faceSize, faceSize),\n",
    "#             flags = cv2.CASCADE_SCALE_IMAGE\n",
    "#             )\n",
    "\n",
    "#             # If a face is detected\n",
    "#             if len(faces):\n",
    "#                 # Convert to gray\n",
    "#                 gImg = cv2.cvtColor(rawImg, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#                 # Resize just the face\n",
    "#                 x,y,w,h = faces[0,:]\n",
    "#                 faceImg = gImg[y:y+h,x:x+w]\n",
    "#                 img = cv2.resize(faceImg,(outImgSize,outImgSize),interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "# #                 cv2.imshow('Image',img)\n",
    "# #                 cv2.waitKey(30)\n",
    "\n",
    "#                 # Flatten\n",
    "#                 l = img.shape[0] * img.shape[1]\n",
    "#                 img = img.reshape(1, l)\n",
    "                \n",
    "#                 # Append to subjectEmotions\n",
    "#                 subjectNames.append(subject)\n",
    "#                 # Append to subjectEmotions\n",
    "#                 subjectEmotions.append(thisEmotion)\n",
    "#                 # Append to subjectFiles\n",
    "#                 subjectFiles.append(imFile) \n",
    "#                 # Append to output matrices\n",
    "#                 subjectImages.append(img[0])\n",
    "                \n",
    "# print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to numpy array    \n",
    "subjectNames = np.asarray(subjectNames)\n",
    "subjectEmotions = np.asarray(subjectEmotions)\n",
    "subjectFiles = np.asarray(subjectFiles)\n",
    "subjectImages = np.asarray(subjectImages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Save the objects to file\n",
    "# import cPickle as pickle\n",
    "# with open('subjectData.pickle','w') as f:\n",
    "#     pickle.dump([subjectNames,subjectEmotions,subjectFiles,subjectImages],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.load(open('subjectData.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    subjectImages, subjectEmotions, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the top 50 eigenfaces from 261 faces\n",
      "done in 6.535s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#n_components = len(emotions)\n",
    "n_components = 50\n",
    "\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\"\n",
    "      % (n_components, X_train.shape[0]))\n",
    "t0 = time()\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized',\n",
    "          whiten=True).fit(X_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projecting the input data on the eigenfaces orthonormal basis\n",
      "done in 0.645s\n"
     ]
    }
   ],
   "source": [
    "print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
    "t0 = time()\n",
    "X_train_pca = pca.transform(X_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier to the training set\n",
      "done in 2.476s\n",
      "Best estimator found by grid search:\n",
      "SVC(C=1000.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import neighbors\n",
    "\n",
    "print(\"Fitting the classifier to the training set\")\n",
    "t0 = time()\n",
    "\n",
    "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced',probability=True), param_grid)\n",
    "#clf = GridSearchCV(SVC(kernel='rbf',probability=True), param_grid)\n",
    "\n",
    "\n",
    "# metrics = ['minkowski','euclidean','manhattan'] \n",
    "# weights = ['uniform','distance'] #10.0**np.arange(-5,4)\n",
    "# NN = np.arange(1,30)\n",
    "# #algorithms = ['ball_tree', 'kd_tree', 'brute']\n",
    "# algorithms = ['brute']\n",
    "# param_grid = dict(metric=metrics,weights=weights,n_neighbors=NN,algorithm=algorithms)\n",
    "# clf = GridSearchCV(neighbors.KNeighborsClassifier(), param_grid)\n",
    "\n",
    "clf = clf.fit(X_train_pca, Y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      anger       0.58      0.79      0.67        14\n",
      "    disgust       1.00      0.62      0.77        16\n",
      "        joy       1.00      1.00      1.00        12\n",
      "    sadness       0.57      0.57      0.57        14\n",
      "   surprise       0.73      0.80      0.76        10\n",
      "\n",
      "avg / total       0.78      0.74      0.75        66\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X_test_pca = pca.transform(X_test)\n",
    "Y_pred = clf.predict(X_test_pca)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.008268320094050548,\n",
       " 0.015326223793859502,\n",
       " 0.9297272868249704,\n",
       " 0.02745170594234155,\n",
       " 0.019226463344777858]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = clf.predict_proba(X_test_pca)\n",
    "print(results.shape)\n",
    "map(float,(results[4,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disgust' 'disgust' 'sadness' 'joy' 'joy']\n",
      "['disgust' 'disgust' 'anger' 'joy' 'joy']\n",
      "['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "print(Y_pred[0:5])\n",
    "print(Y_test[0:5])\n",
    "print(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Novel image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# OpenCV face classifier\n",
    "cascPath = '/usr/local/Cellar/opencv/2.4.13.2/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml'\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "#inFace = '/Users/abock/Desktop/InsightProject/sad_keanu_Big.png'\n",
    "#inFace = '/Users/abock/Desktop/InsightProject/happy_keanu.png'\n",
    "inFace = '/Users/abock/Desktop/InsightProject/crazy-person.png'\n",
    "#inFace = '/Users/abock/Desktop/InsightProject/happy_guy.png'\n",
    "#inFace = '/Users/abock/Desktop/InsightProject/happy_girl.png'\n",
    "#inFace = '/Users/abock/Desktop/InsightProject/smiling-women.jpg'\n",
    "#inFace = '/Users/abock/Desktop/InsightProject/sadGoslingMeme.jpg'\n",
    "\n",
    "\n",
    "#inFace = '/Users/abock/Desktop/InsightProject/aia_sadness_547.png'\n",
    "# inFace = '/Users/abock/Desktop/InsightProject/aia_anger_93.png'\n",
    "#inFace = '/Users/abock/Desktop/InsightProject/malcolm_joy_104.png'\n",
    "\n",
    "\n",
    "# Load the image\n",
    "rawImg = cv2.imread(inFace)\n",
    "\n",
    "#img = rawImg\n",
    "\n",
    "img = cv2.resize(rawImg,(500,500),interpolation = cv2.INTER_CUBIC) \n",
    "\n",
    "cv2.imshow('Image',img)\n",
    "cv2.waitKey(30)\n",
    "\n",
    "# Detect faces in the image\n",
    "faces = faceCascade.detectMultiScale(\n",
    "    img,\n",
    "    scaleFactor=scaleSize,\n",
    "    minNeighbors=1,\n",
    "    minSize=(10, 10),\n",
    "    flags = cv2.CASCADE_SCALE_IMAGE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[201, 124,  78,  78],\n",
       "       [110, 110, 209, 209],\n",
       "       [336, 417,  69,  69],\n",
       "       [353, 407,  53,  53]], dtype=int32)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the largest face\n",
    "i=np.where(faces[:,2] == faces[:,2].max())\n",
    "faces = faces[i,:]\n",
    "faces = faces[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([110, 110, 209, 209], dtype=int32)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "  \n",
    "# If a face is detected\n",
    "if len(faces):\n",
    "    # Convert to gray\n",
    "    gImg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Resize just the face\n",
    "    x,y,w,h = faces[0,:]\n",
    "    faceImg = gImg[y:y+h,x:x+w]\n",
    "    img = cv2.resize(faceImg,(outImgSize,outImgSize),interpolation = cv2.INTER_CUBIC)\n",
    "    \n",
    "    # Flatten\n",
    "    l = img.shape[0] * img.shape[1]\n",
    "    img = img.reshape(1, l)\n",
    "\n",
    "testFace = np.asarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow('Image',faceImg)\n",
    "cv2.waitKey(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joy']\n"
     ]
    }
   ],
   "source": [
    "X_test_pca = pca.transform(testFace)\n",
    "Y_pred = clf.predict(X_test_pca)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03690627643478289,\n",
       " 0.1352293835173045,\n",
       " 0.5230312037500001,\n",
       " 0.07338635010422995,\n",
       " 0.23144678619368253]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(emotions)\n",
    "probs = clf.predict_proba(X_test_pca)\n",
    "map(float,(probs[0,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joy']\n",
      "['disgust']\n",
      "['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "print(Y_pred[0:5])\n",
    "print(Y_test[0:5])\n",
    "print(emotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show cropped face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy\n",
    "import time\n",
    "inDir = '/Users/abock/Desktop/InsightProject/'\n",
    "cascPath = '/usr/local/Cellar/opencv/2.4.13.2/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml'\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "rawImg = cv2.imread(inDir + 'happy-person.png')\n",
    "h,w = rawImg.shape[:2]\n",
    "#imS = cv2.resize(rawImg, (w*2,h*2),interpolation = cv2.INTER_CUBIC)\n",
    "imS = rawImg\n",
    "gray = cv2.cvtColor(imS, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect faces in the image\n",
    "faces = faceCascade.detectMultiScale(\n",
    "    imS,\n",
    "    scaleFactor=1.1,\n",
    "    minNeighbors=1,\n",
    "    minSize=(200, 200),\n",
    "    flags = cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(imS, (x, y), (x+w, y+h), (0, 255, 0), 4)\n",
    "    face = imS[y:y+h,x:x+w]\n",
    "    cv2.imshow('Face',face)\n",
    "    cv2.waitKey(30)\n",
    "\n",
    "time.sleep(3)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record Movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "\n",
    "outDir = '/Users/abock/Desktop/faceMovie/'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "ct = 0\n",
    "while ct < 50:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',gray)\n",
    "    cv2.waitKey(30)\n",
    "    \n",
    "    # save the frames\n",
    "    ct = ct + 1\n",
    "    outFrame = os.path.join(outDir, \"frame\" + str(ct) + \".jpeg\")\n",
    "    cv2.imwrite(outFrame,gray)\n",
    "    \n",
    "    # save a movie\n",
    "    cv2.write(outVid,gray)\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "Y = iris.target\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "logreg.fit(X, Y)\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2 \n",
    "import urllib\n",
    "import os\n",
    "\n",
    "def saveURLimages(url,outDir):\n",
    "    req = urllib2.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    html = urllib2.urlopen(req).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    # get images\n",
    "    images = [img for img in soup.findAll('img')]\n",
    "    print (str(len(images)) + \" images found.\")\n",
    "    print 'Downloading images to: ' + outDir\n",
    "    \n",
    "    # download images\n",
    "    image_links = [each.get('src') for each in images]\n",
    "    #print(image_links)\n",
    "    for each in image_links:\n",
    "        if not each is None:\n",
    "            if each[0:4] == 'http':\n",
    "                #print(each)\n",
    "                filename=each.split('/')[-1]\n",
    "                urllib.urlretrieve(each,os.path.join(outDir,filename + '.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save happy faces (Google Images)\n",
    "happyPerson = 'https://www.google.com/search?site=&tbm=isch&source=hp&biw=1680&bih=949&q=happy+person&oq=happy+person&gs_l=img.3..0l10.15311.17067.0.17213.12.7.0.5.5.0.82.450.7.7.0....0...1ac.1.64.img..0.12.461.FpA8-IvoCOk'\n",
    "url = happyPerson\n",
    "outDir = '/Users/abock/Desktop/HappyFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "saveURLimages(url,outDir)\n",
    "\n",
    "# Save sad faces (Google Images)\n",
    "sadPerson = 'https://www.google.com/search?site=&tbm=isch&source=hp&biw=840&bih=949&q=sad+person&oq=sad+person&gs_l=img.3..0l10.732.2268.0.2413.10.7.0.3.3.0.337.1010.0j1j2j1.4.0....0...1ac.1.64.img..3.7.1015.rkrdqlI1p-k'\n",
    "url = sadPerson\n",
    "outDir = '/Users/abock/Desktop/SadFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "saveURLimages(url,outDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save happy faces (Bing Images)\n",
    "happyPerson = 'https://www.bing.com/images/search?q=happy%20person&qs=n&form=QBIR&pq=happy%20person&sc=8-12&sp=-1&sk='\n",
    "url = happyPerson\n",
    "outDir = '/Users/abock/Desktop/HappyFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "saveURLimages(url,outDir)\n",
    "\n",
    "# Save sad faces (Bing Images)\n",
    "sadPerson = 'https://www.bing.com/images/search?q=sad+person&qs=n&form=QBILPG&pq=sad+person&sc=8-8&sp=-1&sk='\n",
    "url = sadPerson\n",
    "outDir = '/Users/abock/Desktop/SadFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "saveURLimages(url,outDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Happy Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save happy faces (Yahoo Images)\n",
    "outDir = '/Users/abock/Desktop/HappyFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "    \n",
    "happyPerson = 'https://images.search.yahoo.com/search/images;_ylt=AwrTcXdqJYFYvE0A0juLuLkF?p=happy+person&ei=UTF-8&iscqry=&fr=sfp'\n",
    "url = happyPerson\n",
    "saveURLimages(url,outDir)\n",
    "\n",
    "happyHumanFace = 'https://images.search.yahoo.com/search/images;_ylt=AwrTcXyAJoFYWHMApB6JzbkF;_ylu=X3oDMTBsZ29xY3ZzBHNlYwNzZWFyY2gEc2xrA2J1dHRvbg--;_ylc=X1MDOTYwNjI4NTcEX3IDMgRhY3RuA2NsawRiY2sDNmltaTE4bGM2N29ybyUyNmIlM0QzJTI2cyUzRDI5BGNzcmNwdmlkAzNzeFdhVEl3Tmk1cFdrRkZXR1BqZUFkSE5qVXVPQUFBQUFDdHdqNncEZnIDc2ZwBGZyMgNzYS1ncARncHJpZANLQm5lbEt4X1JOeUlzNXY5WC40cVRBBG10ZXN0aWQDbnVsbARuX3N1Z2cDMTAEb3JpZ2luA2ltYWdlcy5zZWFyY2gueWFob28uY29tBHBvcwMwBHBxc3RyAwRwcXN0cmwDBHFzdHJsAzE2BHF1ZXJ5A2hhcHB5IGh1bWFuIGZhY2UEdF9zdG1wAzE0ODQ4NTkwNzYEdnRlc3RpZANudWxs?gprid=KBnelKx_RNyIs5v9X.4qTA&pvid=3sxWaTIwNi5pWkFFWGPjeAdHNjUuOAAAAACtwj6w&p=happy+human+face&fr=sfp&fr2=sb-top-images.search.yahoo.com&ei=UTF-8&n=60&x=wrt'\n",
    "url = happyHumanFace\n",
    "saveURLimages(url,outDir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sad Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output directory\n",
    "outDir = '/Users/abock/Desktop/SadFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "   \n",
    "# Yahoo Images\n",
    "sadPerson = 'https://images.search.yahoo.com/search/images;_ylt=AwrTcXZ6JYFYl68AvSGJzbkF;_ylu=X3oDMTBsZ29xY3ZzBHNlYwNzZWFyY2gEc2xrA2J1dHRvbg--;_ylc=X1MDOTYwNjI4NTcEX3IDMgRhY3RuA2NsawRiY2sDNmltaTE4bGM2N29ybyUyNmIlM0QzJTI2cyUzRDI5BGNzcmNwdmlkA3VBdHlfekl3Tmk1cFdrRkZXR1BqZUFDaE5qVXVPQUFBQUFDZUhiR3oEZnIDc2ZwBGZyMgNzYS1ncARncHJpZANnb3NOVG13V1RTdVc1MXA5ZmhMZGlBBG10ZXN0aWQDbnVsbARuX3N1Z2cDMTAEb3JpZ2luA2ltYWdlcy5zZWFyY2gueWFob28uY29tBHBvcwMwBHBxc3RyAwRwcXN0cmwDBHFzdHJsAzEwBHF1ZXJ5A3NhZCBwZXJzb24EdF9zdG1wAzE0ODQ4NTg3NjAEdnRlc3RpZANudWxs?gprid=gosNTmwWTSuW51p9fhLdiA&pvid=uAty_zIwNi5pWkFFWGPjeAChNjUuOAAAAACeHbGz&p=sad+person&fr=sfp&fr2=sb-top-images.search.yahoo.com&ei=UTF-8&n=60&x=wrt'\n",
    "url = sadPerson\n",
    "saveURLimages(url,outDir)\n",
    "\n",
    "sadHumanFace = 'https://images.search.yahoo.com/search/images;_ylt=AwrTcXmHJYFY_lwACdOJzbkF;_ylu=X3oDMTBsZ29xY3ZzBHNlYwNzZWFyY2gEc2xrA2J1dHRvbg--;_ylc=X1MDOTYwNjI4NTcEX3IDMgRhY3RuA2NsawRiY2sDNmltaTE4bGM2N29ybyUyNmIlM0QzJTI2cyUzRDI5BGNzcmNwdmlkA2N3Q2RJREl3Tmk1cFdrRkZXR1BqZUFUVU5qVXVPQUFBQUFDZTZzVXEEZnIDc2ZwBGZyMgNzYS1ncARncHJpZANKZHp4ZFBPMVFfSzFlYlZTTkpycGhBBG10ZXN0aWQDbnVsbARuX3N1Z2cDMTAEb3JpZ2luA2ltYWdlcy5zZWFyY2gueWFob28uY29tBHBvcwMwBHBxc3RyAwRwcXN0cmwDBHFzdHJsAzE0BHF1ZXJ5A3NhZCBodW1hbiBmYWNlBHRfc3RtcAMxNDg0ODU5MDA5BHZ0ZXN0aWQDbnVsbA--?gprid=JdzxdPO1Q_K1ebVSNJrphA&pvid=cwCdIDIwNi5pWkFFWGPjeATUNjUuOAAAAACe6sUq&p=sad+human+face&fr=sfp&fr2=sb-top-images.search.yahoo.com&ei=UTF-8&n=60&x=wrt'\n",
    "url = sadHumanFace\n",
    "saveURLimages(url,outDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
