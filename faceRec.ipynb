{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# faceRec\n",
    "\n",
    "This IPython notebook will recognize a face in a video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "\n",
    "| Yale         | JAFFE        | FERG         | CK+          |\n",
    "| ------------ | ------------ | ------------ | ------------ |\n",
    "| 15 subjects  | 60 subjects  | 6  subjects  | 123 subjects |\n",
    "| 1 female     | 60 female    | 3  female    | 85 female    |\n",
    "| 1 image each | variable #   | variable #   | variable #   | \n",
    "| ------------ | ------------ | ------------ | ------------ |\n",
    "| happy        | happy        | happy        | happy        |\n",
    "| sad          | sad          | sad          | sad          |\n",
    "| surprise     | surprise     | surprise     | surprise     |\n",
    "| neutral      |              | neutral      | neutral      |\n",
    "|              | anger        | anger        | anger        |\n",
    "|              | disgust      | disgust      | disgust      |\n",
    "|              | fear         | fear         | fear         |\n",
    "\n",
    "\n",
    "CK+ Notes:\n",
    "\n",
    "4) The Emotion coded files (Emotion_labels.zip) - ONLY 327 of the 593 sequences have emotion sequences. This is because these are the only ones the fit the prototypic definition. Like the FACS files, there is only 1 Emotion file for each sequence which is the last frame (the peak frame). There should be only one entry and the number will range from 0-7 (i.e. 0=neutral, 1=anger, 2=contempt, 3=disgust, 4=fear, 5=happy, 6=sadness, 7=surprise). N.B there is only 327 files- IF THERE IS NO FILE IT MEANS THAT THERE IS NO EMOTION LABEL (sorry to be explicit but this will avoid confusion).\n",
    "\n",
    "\n",
    "JAFFE Notes:\n",
    "\n",
    "-------------- Semantic Ratings Data (Fear Excluded) --------------\n",
    "\n",
    "This rating experiment excluded fear images and and the fear adjective\n",
    "from the ratings. We did this because we thought that the expressors\n",
    "were not good at posing fear. There is some evidence in the scientific\n",
    "literature that fear may be processed differently from the other basic\n",
    "facial expressions.\n",
    "\n",
    "Please note that there are less subjects than the other experiment.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CK+ images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the faces and emotion files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Set paths, variable values\n",
    "dbDir = '/Users/abock/Desktop/InsightProject/faceDatabases/CK+/'\n",
    "emoDir = '/Users/abock/Desktop/InsightProject/faceDatabases/CK+/Emotion/'\n",
    "emotions = ['anger','disgust','fear','joy','neutral','sadness','surprise']\n",
    "emoNums = [1,3,4,5,0,6,7]\n",
    "\n",
    "\n",
    "subjectNames = []\n",
    "subjectFiles = []\n",
    "subjectEmotions = []\n",
    "\n",
    "# Find the images and emotion labels\n",
    "for subDir in glob.glob(emoDir + 'S*/'):\n",
    "    subject = os.path.basename(os.path.dirname(subDir))\n",
    "    # Append to subjectEmotions\n",
    "    subjectNames.append(subject)\n",
    "    for faceDir in glob.glob(subDir + '*/'):\n",
    "        dirNum = os.path.basename(os.path.dirname(faceDir))\n",
    "        for thisFile in glob.glob(faceDir + '*.txt'):\n",
    "            imDir = os.path.join(dbDir,subject,dirNum)\n",
    "            imFile = os.path.basename(thisFile[:-12])\n",
    "            with open(thisFile) as f:\n",
    "                for line in f:\n",
    "                    line = map(float, line.split())\n",
    "                    lineInt = int(line[0])\n",
    "                    if lineInt != 2 and lineInt !=4: # 2 is contempt\n",
    "                        thisEmotion = emotions[emoNums.index(int(line[0]))]\n",
    "                        # Append to subjectEmotions\n",
    "                        subjectEmotions.append(thisEmotion)\n",
    "                        # Append to subjectFiles\n",
    "                        subjectFiles.append(os.path.join(imDir,imFile + '.png'))\n",
    "    \n",
    "subjectNames = np.asarray(subjectNames)\n",
    "subjectEmotions = np.asarray(subjectEmotions)\n",
    "subjectFiles = np.asarray(subjectFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the face images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input images...\n",
      "done in 12.839s\n"
     ]
    }
   ],
   "source": [
    "import cv2     \n",
    "from time import time\n",
    "\n",
    "# OpenCV face classifier\n",
    "cascPath = '/usr/local/Cellar/opencv/2.4.13.2/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml'\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "# Face processing defaults\n",
    "outImgSize = 500\n",
    "scaleSize = 1.05\n",
    "numNeighbors = 3\n",
    "faceSize = 150\n",
    "\n",
    "print(\"Loading input images...\")\n",
    "t0 = time()\n",
    "\n",
    "# Loop through all images in the input directory\n",
    "subjectImages = []\n",
    "\n",
    "# Loop through images\n",
    "for inFace in subjectFiles:\n",
    "    # Load the image\n",
    "    rawImg = cv2.imread(inFace)\n",
    "\n",
    "    # Detect faces in the image\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "    rawImg,\n",
    "    scaleFactor=scaleSize,\n",
    "    minNeighbors=numNeighbors,\n",
    "    minSize=(faceSize, faceSize),\n",
    "    flags = cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "\n",
    "    # If a face is detected\n",
    "    if len(faces):\n",
    "        # Convert to gray\n",
    "        gImg = cv2.cvtColor(rawImg, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Resize just the face\n",
    "        x,y,w,h = faces[0,:]\n",
    "        faceImg = gImg[y:y+h,x:x+w]\n",
    "        img = cv2.resize(faceImg,(outImgSize,outImgSize),interpolation = cv2.INTER_CUBIC)\n",
    "        \n",
    "        # Flatten\n",
    "        l = img.shape[0] * img.shape[1]\n",
    "        img = img.reshape(1, l)\n",
    "\n",
    "        # Append to output matrices\n",
    "        subjectImages.append(img[0])\n",
    "            \n",
    "subjectImages = np.asarray(subjectImages)\n",
    "\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    subjectImages, subjectEmotions, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the top 5 eigenfaces from 227 faces\n",
      "done in 4.722s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from time import time \n",
    "\n",
    "\n",
    "#n_components = len(emotions)\n",
    "n_components = 5\n",
    "\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\"\n",
    "      % (n_components, X_train.shape[0]))\n",
    "t0 = time()\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized',\n",
    "          whiten=True).fit(X_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projecting the input data on the eigenfaces orthonormal basis\n",
      "done in 0.438s\n"
     ]
    }
   ],
   "source": [
    "print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
    "t0 = time()\n",
    "X_train_pca = pca.transform(X_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier to the training set\n",
      "done in 2.550s\n",
      "Best estimator found by grid search:\n",
      "SVC(C=10000.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.0001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import neighbors\n",
    "\n",
    "print(\"Fitting the classifier to the training set\")\n",
    "t0 = time()\n",
    "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)\n",
    "\n",
    "# metrics = ['minkowski','euclidean','manhattan'] \n",
    "# weights = ['uniform','distance'] #10.0**np.arange(-5,4)\n",
    "# NN = np.arange(1,30)\n",
    "# #algorithms = ['ball_tree', 'kd_tree', 'brute']\n",
    "# algorithms = ['brute']\n",
    "# param_grid = dict(metric=metrics,weights=weights,n_neighbors=NN,algorithm=algorithms)\n",
    "# clf = GridSearchCV(neighbors.KNeighborsClassifier(), param_grid)\n",
    "\n",
    "clf = clf.fit(X_train_pca, Y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      anger       0.38      0.38      0.38         8\n",
      "    disgust       0.62      0.42      0.50        12\n",
      "        joy       0.50      0.58      0.54        12\n",
      "    sadness       0.38      0.50      0.43         6\n",
      "   surprise       0.89      0.89      0.89        19\n",
      "\n",
      "avg / total       0.63      0.61      0.61        57\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X_test_pca = pca.transform(X_test)\n",
    "Y_pred = clf.predict(X_test_pca)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Novel image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[142, 164, 353, 353]], dtype=int32)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# OpenCV face classifier\n",
    "cascPath = '/usr/local/Cellar/opencv/2.4.13.2/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml'\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "#inFace = '/Users/abock/Desktop/InsightProject/sad_keanu_Big.png'\n",
    "#inFace = '/Users/abock/Desktop/InsightProject/happy-person.png'\n",
    "inFace = '/Users/abock/Desktop/InsightProject/happy_girl.png'\n",
    "\n",
    "# Load the image\n",
    "rawImg = cv2.imread(inFace)\n",
    "\n",
    "img = rawImg\n",
    "\n",
    "cv2.imshow('Image',rawImg)\n",
    "#img = cv2.resize(rawImg,(250,250),interpolation = cv2.INTER_CUBIC)    \n",
    "\n",
    "# Detect faces in the image\n",
    "faces = faceCascade.detectMultiScale(\n",
    "    img,\n",
    "    scaleFactor=scaleSize,\n",
    "    minNeighbors=numNeighbors,\n",
    "    minSize=(10, 10),\n",
    "    flags = cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "  \n",
    "# If a face is detected\n",
    "if len(faces):\n",
    "    # Convert to gray\n",
    "    gImg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Resize just the face\n",
    "    x,y,w,h = faces[0,:]\n",
    "    faceImg = gImg[y:y+h,x:x+w]\n",
    "    img = cv2.resize(faceImg,(outImgSize,outImgSize),interpolation = cv2.INTER_CUBIC)\n",
    "    \n",
    "    # Flatten\n",
    "    l = img.shape[0] * img.shape[1]\n",
    "    img = img.reshape(1, l)\n",
    "\n",
    "Keanu = np.asarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_pca = pca.transform(Keanu)\n",
    "Y_pred = clf.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['joy'], \n",
       "      dtype='|S8')"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show cropped face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy\n",
    "import time\n",
    "inDir = '/Users/abock/Desktop/InsightProject/'\n",
    "cascPath = '/usr/local/Cellar/opencv/2.4.13.2/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml'\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "rawImg = cv2.imread(inDir + 'happy-person.png')\n",
    "h,w = rawImg.shape[:2]\n",
    "#imS = cv2.resize(rawImg, (w*2,h*2),interpolation = cv2.INTER_CUBIC)\n",
    "imS = rawImg\n",
    "gray = cv2.cvtColor(imS, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect faces in the image\n",
    "faces = faceCascade.detectMultiScale(\n",
    "    imS,\n",
    "    scaleFactor=1.1,\n",
    "    minNeighbors=1,\n",
    "    minSize=(200, 200),\n",
    "    flags = cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(imS, (x, y), (x+w, y+h), (0, 255, 0), 4)\n",
    "    face = imS[y:y+h,x:x+w]\n",
    "    cv2.imshow('Face',face)\n",
    "    cv2.waitKey(30)\n",
    "\n",
    "time.sleep(3)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[126, 111, 243, 243]], dtype=int32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy\n",
    "import glob\n",
    "\n",
    "inDir = '/Users/abock/Desktop/InsightProject/FERG_DB_256/aia/aia_joy/'\n",
    "cascPath = '/usr/local/Cellar/opencv/2.4.13.2/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml'\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "allFiles = glob.glob(inDir + '*png')\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "outFile = '/Users/abock/Desktop/aia_joy.avi'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "\n",
    "vid = None\n",
    "is_color = True\n",
    "ct = 0\n",
    "for inFace in allFiles:\n",
    "    ct = ct + 1\n",
    "    rawImg = cv2.imread(inFace)\n",
    "    h,w = rawImg.shape[:2]\n",
    "    #imS = cv2.resize(rawImg, (w*3,h*3),interpolation = cv2.INTER_CUBIC)\n",
    "    imS = rawImg\n",
    "    gray = cv2.cvtColor(imS, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    if vid is None:\n",
    "        vid = cv2.VideoWriter(outFile,fourcc, 30.0, (256,256))\n",
    "\n",
    "    # Detect faces in the image\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=1,\n",
    "        minSize=(150, 150),\n",
    "        flags = cv2.CASCADE_SCALE_IMAGE\n",
    "        )\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(gray, (x, y), (x+w, y+h), (0, 255, 0), 4)\n",
    "        vid.write(gray)\n",
    "        cv2.imshow('Face',gray)\n",
    "        cv2.waitKey(30)\n",
    "    if ct > 10:\n",
    "        break\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import mean,cov,cumsum,dot,linalg,size,flipud,argsort\n",
    "\n",
    "def princomp(A,numpc=0):\n",
    "    # computing eigenvalues and eigenvectors of covariance matrix\n",
    "    M = (A-mean(A.T,axis=1)).T # subtract the mean (along columns)\n",
    "    [latent,coeff] = linalg.eig(cov(M))\n",
    "    p = size(coeff,axis=1)\n",
    "    idx = argsort(latent) # sorting the eigenvalues\n",
    "    idx = idx[::-1]       # in ascending order\n",
    "    # sorting eigenvectors according to the sorted eigenvalues\n",
    "    coeff = coeff[:,idx]\n",
    "    latent = latent[idx] # sorting eigenvalues\n",
    "    if numpc < p and numpc >= 0:\n",
    "            coeff = coeff[:,range(numpc)] # cutting some PCs if needed\n",
    "    score = dot(coeff.T,M) # projection of the data in the new space\n",
    "    return coeff,score,latent\n",
    "\n",
    "from pylab import imread,subplot,imshow,title,gray,figure,show,NullLocator\n",
    "A = rawImg\n",
    "A = mean(A,2) # to get a 2-D array\n",
    "full_pc = size(A,axis=1) # numbers of all the principal components\n",
    "i = 1\n",
    "dist = []\n",
    "\n",
    "step = 10\n",
    "for numpc in range(0,full_pc+step,step): # 0 step*1 step*2 ... full_pc\n",
    "    coeff, score, latent = princomp(A,numpc)\n",
    "    Ar = dot(coeff,score).T+mean(A,axis=0) # image reconstruction\n",
    "    # difference in Frobenius norm\n",
    "    dist.append(linalg.norm(A-Ar,'fro'))\n",
    "    # showing the pics reconstructed with less than 50 PCs\n",
    "    if numpc <= 50:\n",
    "        ax = subplot(2,3,i,frame_on=False)\n",
    "        ax.xaxis.set_major_locator(NullLocator()) # remove ticks\n",
    "        ax.yaxis.set_major_locator(NullLocator())\n",
    "        i += 1 \n",
    "        imshow(Ar)\n",
    "        title('PCs # '+str(numpc))\n",
    "        gray()\n",
    "\n",
    "figure()\n",
    "imshow(A)\n",
    "title('numpc FULL')\n",
    "gray()\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy\n",
    "import time\n",
    "inDir = '/Users/abock/Desktop/faceMovie/'\n",
    "\n",
    "# Detect faces in the image\n",
    "cascPath = '/usr/local/Cellar/opencv/2.4.13.2/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml'\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "for i in range(1,11):\n",
    "    rawImg = cv2.imread(inDir + 'frame' + str(i) + '.jpeg')\n",
    "    imS = rawImg\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "        imS,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=1,\n",
    "        minSize=(100, 100),\n",
    "        flags = cv2.CASCADE_SCALE_IMAGE\n",
    "        )\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(imS, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        cv2.imshow('Face',imS)\n",
    "        cv2.waitKey(30)\n",
    "time.sleep(3)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawImg = cv2.imread('/Users/abock/aia_anger_561.png')\n",
    "imS = rawImg\n",
    "gray = cv2.cvtColor(imS, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "\n",
    "faces = faceCascade.detectMultiScale(\n",
    "    gray,\n",
    "    scaleFactor=1.1,\n",
    "    minNeighbors=1,\n",
    "    minSize=(10, 10),\n",
    "    flags = cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(gray, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    cv2.imshow('gray',gray)\n",
    "    cv2.waitKey(30)\n",
    "    \n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cascPath = '/usr/local/Cellar/opencv/2.4.13.2/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml'\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "rawImg = cv2.imread('/Users/abock/aia_anger_561.png')\n",
    "imS = rawImg\n",
    "gray = cv2.cvtColor(imS, cv2.COLOR_BGR2GRAY)\n",
    "faces = faceCascade.detectMultiScale(\n",
    "    gray,\n",
    "    scaleFactor=1.1,\n",
    "    minNeighbors=1,\n",
    "    minSize=(10, 10),\n",
    "    flags = cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(imS, (x, y), (x+w, y+h), (0, 255, 0), 4)\n",
    "    cv2.imshow('Face',imS)\n",
    "    cv2.waitKey(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record Movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "\n",
    "outDir = '/Users/abock/Desktop/faceMovie/'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "ct = 0\n",
    "while ct < 50:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',gray)\n",
    "    cv2.waitKey(30)\n",
    "    \n",
    "    # save the frames\n",
    "    ct = ct + 1\n",
    "    outFrame = os.path.join(outDir, \"frame\" + str(ct) + \".jpeg\")\n",
    "    cv2.imwrite(outFrame,gray)\n",
    "    \n",
    "    # save a movie\n",
    "    cv2.write(outVid,gray)\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "Y = iris.target\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "logreg.fit(X, Y)\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2 \n",
    "import urllib\n",
    "import os\n",
    "\n",
    "def saveURLimages(url,outDir):\n",
    "    req = urllib2.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    html = urllib2.urlopen(req).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    # get images\n",
    "    images = [img for img in soup.findAll('img')]\n",
    "    print (str(len(images)) + \" images found.\")\n",
    "    print 'Downloading images to: ' + outDir\n",
    "    \n",
    "    # download images\n",
    "    image_links = [each.get('src') for each in images]\n",
    "    #print(image_links)\n",
    "    for each in image_links:\n",
    "        if not each is None:\n",
    "            if each[0:4] == 'http':\n",
    "                #print(each)\n",
    "                filename=each.split('/')[-1]\n",
    "                urllib.urlretrieve(each,os.path.join(outDir,filename + '.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save happy faces (Google Images)\n",
    "happyPerson = 'https://www.google.com/search?site=&tbm=isch&source=hp&biw=1680&bih=949&q=happy+person&oq=happy+person&gs_l=img.3..0l10.15311.17067.0.17213.12.7.0.5.5.0.82.450.7.7.0....0...1ac.1.64.img..0.12.461.FpA8-IvoCOk'\n",
    "url = happyPerson\n",
    "outDir = '/Users/abock/Desktop/HappyFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "saveURLimages(url,outDir)\n",
    "\n",
    "# Save sad faces (Google Images)\n",
    "sadPerson = 'https://www.google.com/search?site=&tbm=isch&source=hp&biw=840&bih=949&q=sad+person&oq=sad+person&gs_l=img.3..0l10.732.2268.0.2413.10.7.0.3.3.0.337.1010.0j1j2j1.4.0....0...1ac.1.64.img..3.7.1015.rkrdqlI1p-k'\n",
    "url = sadPerson\n",
    "outDir = '/Users/abock/Desktop/SadFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "saveURLimages(url,outDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save happy faces (Bing Images)\n",
    "happyPerson = 'https://www.bing.com/images/search?q=happy%20person&qs=n&form=QBIR&pq=happy%20person&sc=8-12&sp=-1&sk='\n",
    "url = happyPerson\n",
    "outDir = '/Users/abock/Desktop/HappyFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "saveURLimages(url,outDir)\n",
    "\n",
    "# Save sad faces (Bing Images)\n",
    "sadPerson = 'https://www.bing.com/images/search?q=sad+person&qs=n&form=QBILPG&pq=sad+person&sc=8-8&sp=-1&sk='\n",
    "url = sadPerson\n",
    "outDir = '/Users/abock/Desktop/SadFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "saveURLimages(url,outDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Happy Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save happy faces (Yahoo Images)\n",
    "outDir = '/Users/abock/Desktop/HappyFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "    \n",
    "happyPerson = 'https://images.search.yahoo.com/search/images;_ylt=AwrTcXdqJYFYvE0A0juLuLkF?p=happy+person&ei=UTF-8&iscqry=&fr=sfp'\n",
    "url = happyPerson\n",
    "saveURLimages(url,outDir)\n",
    "\n",
    "happyHumanFace = 'https://images.search.yahoo.com/search/images;_ylt=AwrTcXyAJoFYWHMApB6JzbkF;_ylu=X3oDMTBsZ29xY3ZzBHNlYwNzZWFyY2gEc2xrA2J1dHRvbg--;_ylc=X1MDOTYwNjI4NTcEX3IDMgRhY3RuA2NsawRiY2sDNmltaTE4bGM2N29ybyUyNmIlM0QzJTI2cyUzRDI5BGNzcmNwdmlkAzNzeFdhVEl3Tmk1cFdrRkZXR1BqZUFkSE5qVXVPQUFBQUFDdHdqNncEZnIDc2ZwBGZyMgNzYS1ncARncHJpZANLQm5lbEt4X1JOeUlzNXY5WC40cVRBBG10ZXN0aWQDbnVsbARuX3N1Z2cDMTAEb3JpZ2luA2ltYWdlcy5zZWFyY2gueWFob28uY29tBHBvcwMwBHBxc3RyAwRwcXN0cmwDBHFzdHJsAzE2BHF1ZXJ5A2hhcHB5IGh1bWFuIGZhY2UEdF9zdG1wAzE0ODQ4NTkwNzYEdnRlc3RpZANudWxs?gprid=KBnelKx_RNyIs5v9X.4qTA&pvid=3sxWaTIwNi5pWkFFWGPjeAdHNjUuOAAAAACtwj6w&p=happy+human+face&fr=sfp&fr2=sb-top-images.search.yahoo.com&ei=UTF-8&n=60&x=wrt'\n",
    "url = happyHumanFace\n",
    "saveURLimages(url,outDir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sad Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output directory\n",
    "outDir = '/Users/abock/Desktop/SadFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "   \n",
    "# Yahoo Images\n",
    "sadPerson = 'https://images.search.yahoo.com/search/images;_ylt=AwrTcXZ6JYFYl68AvSGJzbkF;_ylu=X3oDMTBsZ29xY3ZzBHNlYwNzZWFyY2gEc2xrA2J1dHRvbg--;_ylc=X1MDOTYwNjI4NTcEX3IDMgRhY3RuA2NsawRiY2sDNmltaTE4bGM2N29ybyUyNmIlM0QzJTI2cyUzRDI5BGNzcmNwdmlkA3VBdHlfekl3Tmk1cFdrRkZXR1BqZUFDaE5qVXVPQUFBQUFDZUhiR3oEZnIDc2ZwBGZyMgNzYS1ncARncHJpZANnb3NOVG13V1RTdVc1MXA5ZmhMZGlBBG10ZXN0aWQDbnVsbARuX3N1Z2cDMTAEb3JpZ2luA2ltYWdlcy5zZWFyY2gueWFob28uY29tBHBvcwMwBHBxc3RyAwRwcXN0cmwDBHFzdHJsAzEwBHF1ZXJ5A3NhZCBwZXJzb24EdF9zdG1wAzE0ODQ4NTg3NjAEdnRlc3RpZANudWxs?gprid=gosNTmwWTSuW51p9fhLdiA&pvid=uAty_zIwNi5pWkFFWGPjeAChNjUuOAAAAACeHbGz&p=sad+person&fr=sfp&fr2=sb-top-images.search.yahoo.com&ei=UTF-8&n=60&x=wrt'\n",
    "url = sadPerson\n",
    "saveURLimages(url,outDir)\n",
    "\n",
    "sadHumanFace = 'https://images.search.yahoo.com/search/images;_ylt=AwrTcXmHJYFY_lwACdOJzbkF;_ylu=X3oDMTBsZ29xY3ZzBHNlYwNzZWFyY2gEc2xrA2J1dHRvbg--;_ylc=X1MDOTYwNjI4NTcEX3IDMgRhY3RuA2NsawRiY2sDNmltaTE4bGM2N29ybyUyNmIlM0QzJTI2cyUzRDI5BGNzcmNwdmlkA2N3Q2RJREl3Tmk1cFdrRkZXR1BqZUFUVU5qVXVPQUFBQUFDZTZzVXEEZnIDc2ZwBGZyMgNzYS1ncARncHJpZANKZHp4ZFBPMVFfSzFlYlZTTkpycGhBBG10ZXN0aWQDbnVsbARuX3N1Z2cDMTAEb3JpZ2luA2ltYWdlcy5zZWFyY2gueWFob28uY29tBHBvcwMwBHBxc3RyAwRwcXN0cmwDBHFzdHJsAzE0BHF1ZXJ5A3NhZCBodW1hbiBmYWNlBHRfc3RtcAMxNDg0ODU5MDA5BHZ0ZXN0aWQDbnVsbA--?gprid=JdzxdPO1Q_K1ebVSNJrphA&pvid=cwCdIDIwNi5pWkFFWGPjeATUNjUuOAAAAACe6sUq&p=sad+human+face&fr=sfp&fr2=sb-top-images.search.yahoo.com&ei=UTF-8&n=60&x=wrt'\n",
    "url = sadHumanFace\n",
    "saveURLimages(url,outDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
