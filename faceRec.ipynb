{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# faceRec\n",
    "\n",
    "This IPython notebook will recognize a face in a video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "The following was installed on OSX 10.10.5 (Yosemite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homebrew\n",
    "\n",
    "Install homebrew (http://brew.sh/) if not already installed\n",
    "\n",
    "**Homebrew maintenance**<br>\n",
    "```$ brew doctor\n",
    "$ brew update\n",
    "$ brew upgrade```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV\n",
    "```$ brew tap homebrew/science\n",
    "$ brew install opencv```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy\n",
    "```$ pip install numpy```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python path\n",
    "\n",
    "**Edit your .bash_profile in your home directory to include:**<br> \n",
    "```export PYTHONPATH=/usr/local/lib/python2.7/site-packages:$PYTHONPATH```\n",
    "\n",
    "**Save and reload the bash profile**<br>\n",
    "```$ source .bash_profile```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the required modules\n",
    "import cv2\n",
    "import numpy\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get user supplied values\n",
    "imagePath = '/Users/abock/Downloads/Barack_Obama_family_portrait_2011.jpg'\n",
    "cascPath = '/usr/local/Cellar/opencv/2.4.13.2/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the haar cascade\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the image\n",
    "image = cv2.imread(imagePath)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imS = cv2.resize(image, (960, 540))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv2.imshow('raw',imS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(imS, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('gray',gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Detect faces in the image\n",
    "faces = faceCascade.detectMultiScale(\n",
    "    gray,\n",
    "    scaleFactor=2,\n",
    "    minNeighbors=1,\n",
    "    minSize=(75, 75),\n",
    "    flags = cv2.CASCADE_SCALE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for (x, y, w, h) in faces:\n",
    "    cv2.imshow('gray',gray)\n",
    "    cv2.rectangle(gray, (x, y), (x+w, y+h), (0, 255, 0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2 \n",
    "import urllib\n",
    "import os\n",
    "\n",
    "def saveURLimages(url,outDir):\n",
    "    req = urllib2.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    html = urllib2.urlopen(req).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    # get images\n",
    "    images = [img for img in soup.findAll('img')]\n",
    "    print (str(len(images)) + \" images found.\")\n",
    "    print 'Downloading images to: ' + outDir\n",
    "    \n",
    "    # download images\n",
    "    image_links = [each.get('src') for each in images]\n",
    "    #print(image_links)\n",
    "    for each in image_links:\n",
    "        if not each is None:\n",
    "            if each[0:4] == 'http':\n",
    "                #print(each)\n",
    "                filename=each.split('/')[-1]\n",
    "                urllib.urlretrieve(each,os.path.join(outDir,filename + '.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save happy faces (Google Images)\n",
    "happyPerson = 'https://www.google.com/search?site=&tbm=isch&source=hp&biw=1680&bih=949&q=happy+person&oq=happy+person&gs_l=img.3..0l10.15311.17067.0.17213.12.7.0.5.5.0.82.450.7.7.0....0...1ac.1.64.img..0.12.461.FpA8-IvoCOk'\n",
    "url = happyPerson\n",
    "outDir = '/Users/abock/Desktop/HappyFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "saveURLimages(url,outDir)\n",
    "\n",
    "# Save sad faces (Google Images)\n",
    "sadPerson = 'https://www.google.com/search?site=&tbm=isch&source=hp&biw=840&bih=949&q=sad+person&oq=sad+person&gs_l=img.3..0l10.732.2268.0.2413.10.7.0.3.3.0.337.1010.0j1j2j1.4.0....0...1ac.1.64.img..3.7.1015.rkrdqlI1p-k'\n",
    "url = sadPerson\n",
    "outDir = '/Users/abock/Desktop/SadFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "saveURLimages(url,outDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save happy faces (Bing Images)\n",
    "happyPerson = 'https://www.bing.com/images/search?q=happy%20person&qs=n&form=QBIR&pq=happy%20person&sc=8-12&sp=-1&sk='\n",
    "url = happyPerson\n",
    "outDir = '/Users/abock/Desktop/HappyFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "saveURLimages(url,outDir)\n",
    "\n",
    "# Save sad faces (Bing Images)\n",
    "sadPerson = 'https://www.bing.com/images/search?q=sad+person&qs=n&form=QBILPG&pq=sad+person&sc=8-8&sp=-1&sk='\n",
    "url = sadPerson\n",
    "outDir = '/Users/abock/Desktop/SadFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "saveURLimages(url,outDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Happy Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save happy faces (Yahoo Images)\n",
    "outDir = '/Users/abock/Desktop/HappyFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "    \n",
    "happyPerson = 'https://images.search.yahoo.com/search/images;_ylt=AwrTcXdqJYFYvE0A0juLuLkF?p=happy+person&ei=UTF-8&iscqry=&fr=sfp'\n",
    "url = happyPerson\n",
    "saveURLimages(url,outDir)\n",
    "\n",
    "happyHumanFace = 'https://images.search.yahoo.com/search/images;_ylt=AwrTcXyAJoFYWHMApB6JzbkF;_ylu=X3oDMTBsZ29xY3ZzBHNlYwNzZWFyY2gEc2xrA2J1dHRvbg--;_ylc=X1MDOTYwNjI4NTcEX3IDMgRhY3RuA2NsawRiY2sDNmltaTE4bGM2N29ybyUyNmIlM0QzJTI2cyUzRDI5BGNzcmNwdmlkAzNzeFdhVEl3Tmk1cFdrRkZXR1BqZUFkSE5qVXVPQUFBQUFDdHdqNncEZnIDc2ZwBGZyMgNzYS1ncARncHJpZANLQm5lbEt4X1JOeUlzNXY5WC40cVRBBG10ZXN0aWQDbnVsbARuX3N1Z2cDMTAEb3JpZ2luA2ltYWdlcy5zZWFyY2gueWFob28uY29tBHBvcwMwBHBxc3RyAwRwcXN0cmwDBHFzdHJsAzE2BHF1ZXJ5A2hhcHB5IGh1bWFuIGZhY2UEdF9zdG1wAzE0ODQ4NTkwNzYEdnRlc3RpZANudWxs?gprid=KBnelKx_RNyIs5v9X.4qTA&pvid=3sxWaTIwNi5pWkFFWGPjeAdHNjUuOAAAAACtwj6w&p=happy+human+face&fr=sfp&fr2=sb-top-images.search.yahoo.com&ei=UTF-8&n=60&x=wrt'\n",
    "url = happyHumanFace\n",
    "saveURLimages(url,outDir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sad Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output directory\n",
    "outDir = '/Users/abock/Desktop/SadFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "   \n",
    "# Yahoo Images\n",
    "sadPerson = 'https://images.search.yahoo.com/search/images;_ylt=AwrTcXZ6JYFYl68AvSGJzbkF;_ylu=X3oDMTBsZ29xY3ZzBHNlYwNzZWFyY2gEc2xrA2J1dHRvbg--;_ylc=X1MDOTYwNjI4NTcEX3IDMgRhY3RuA2NsawRiY2sDNmltaTE4bGM2N29ybyUyNmIlM0QzJTI2cyUzRDI5BGNzcmNwdmlkA3VBdHlfekl3Tmk1cFdrRkZXR1BqZUFDaE5qVXVPQUFBQUFDZUhiR3oEZnIDc2ZwBGZyMgNzYS1ncARncHJpZANnb3NOVG13V1RTdVc1MXA5ZmhMZGlBBG10ZXN0aWQDbnVsbARuX3N1Z2cDMTAEb3JpZ2luA2ltYWdlcy5zZWFyY2gueWFob28uY29tBHBvcwMwBHBxc3RyAwRwcXN0cmwDBHFzdHJsAzEwBHF1ZXJ5A3NhZCBwZXJzb24EdF9zdG1wAzE0ODQ4NTg3NjAEdnRlc3RpZANudWxs?gprid=gosNTmwWTSuW51p9fhLdiA&pvid=uAty_zIwNi5pWkFFWGPjeAChNjUuOAAAAACeHbGz&p=sad+person&fr=sfp&fr2=sb-top-images.search.yahoo.com&ei=UTF-8&n=60&x=wrt'\n",
    "url = sadPerson\n",
    "saveURLimages(url,outDir)\n",
    "\n",
    "sadHumanFace = 'https://images.search.yahoo.com/search/images;_ylt=AwrTcXmHJYFY_lwACdOJzbkF;_ylu=X3oDMTBsZ29xY3ZzBHNlYwNzZWFyY2gEc2xrA2J1dHRvbg--;_ylc=X1MDOTYwNjI4NTcEX3IDMgRhY3RuA2NsawRiY2sDNmltaTE4bGM2N29ybyUyNmIlM0QzJTI2cyUzRDI5BGNzcmNwdmlkA2N3Q2RJREl3Tmk1cFdrRkZXR1BqZUFUVU5qVXVPQUFBQUFDZTZzVXEEZnIDc2ZwBGZyMgNzYS1ncARncHJpZANKZHp4ZFBPMVFfSzFlYlZTTkpycGhBBG10ZXN0aWQDbnVsbARuX3N1Z2cDMTAEb3JpZ2luA2ltYWdlcy5zZWFyY2gueWFob28uY29tBHBvcwMwBHBxc3RyAwRwcXN0cmwDBHFzdHJsAzE0BHF1ZXJ5A3NhZCBodW1hbiBmYWNlBHRfc3RtcAMxNDg0ODU5MDA5BHZ0ZXN0aWQDbnVsbA--?gprid=JdzxdPO1Q_K1ebVSNJrphA&pvid=cwCdIDIwNi5pWkFFWGPjeATUNjUuOAAAAACe6sUq&p=sad+human+face&fr=sfp&fr2=sb-top-images.search.yahoo.com&ei=UTF-8&n=60&x=wrt'\n",
    "url = sadHumanFace\n",
    "saveURLimages(url,outDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy\n",
    "import time\n",
    "inDir = '/Users/abock/Desktop/InsightProject/'\n",
    "cascPath = '/usr/local/Cellar/opencv/2.4.13.2/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml'\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "rawImg = cv2.imread(inDir + 'sad_keanu_Big.png')\n",
    "h,w = rawImg.shape[:2]\n",
    "#imS = cv2.resize(rawImg, (w*2,h*2),interpolation = cv2.INTER_CUBIC)\n",
    "imS = rawImg\n",
    "gray = cv2.cvtColor(imS, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect faces in the image\n",
    "faces = faceCascade.detectMultiScale(\n",
    "    imS,\n",
    "    scaleFactor=1.1,\n",
    "    minNeighbors=5,\n",
    "    minSize=(250, 250),\n",
    "    flags = cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(imS, (x, y), (x+w, y+h), (0, 255, 0), 4)\n",
    "    cv2.imshow('Face',imS)\n",
    "    cv2.waitKey(30)\n",
    "\n",
    "time.sleep(3)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pylab\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Face defaults\n",
    "cascPath = '/usr/local/Cellar/opencv/2.4.13.2/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml'\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "minFaceSize = 100\n",
    "minNeighborVal = 5\n",
    "\n",
    "# Set paths, variable values\n",
    "faceDir = '/Users/abock/Desktop/InsightProject/FERG_DB_256/'\n",
    "# trainNames = ['aia','bonnie','jules','malcolm','mery']\n",
    "trainNames = ['aia','bonnie']\n",
    "testNames = ['ray']\n",
    "# emotions = ['anger','disgust','fear','joy','neutral','sadness','surprise']\n",
    "emotions = ['joy']\n",
    "\n",
    "# Processing defaults\n",
    "outSize = 500\n",
    "numPCs = 100\n",
    "\n",
    "# Loop through all images in the input directory\n",
    "trainImages = []\n",
    "trainSubs = []\n",
    "trainEmotions = []\n",
    "for subName in trainNames:\n",
    "    for emoName in emotions:\n",
    "        inDir = ''.join((faceDir,subName,'/',subName,'_',emoName,'/'))\n",
    "        for inFace in glob.glob(inDir + '*.png'):\n",
    "            # Load the image\n",
    "            rawImg = cv2.imread(inFace)\n",
    "            # Convert to gray\n",
    "            gImg = cv2.cvtColor(rawImg, cv2.COLOR_BGR2GRAY)\n",
    "            # Resize \n",
    "            img = cv2.resize(gImg,(outSize,outSize),interpolation = cv2.INTER_CUBIC)\n",
    "    \n",
    "            cv2.imshow('Face',img)\n",
    "            cv2.waitKey(1)\n",
    "            # Flatten\n",
    "            l = img.shape[0] * img.shape[1]\n",
    "            img = img.reshape(1, l)\n",
    "            # Append to output matrices\n",
    "            trainImages.append(img[0])\n",
    "            trainSubs.append(subName)\n",
    "            trainEmotions.append(emoName)\n",
    "            \n",
    "trainImages = np.asarray(trainImages)\n",
    "trainSubs = np.asarray(trainSubs)\n",
    "trainEmotions = np.asarray(trainEmotions)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    trainImages, trainSubs, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the top 2 eigenfaces from 2002 faces\n",
      "done in 31.446s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from time import time \n",
    "\n",
    "n_components = 2\n",
    "\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\"\n",
    "      % (n_components, X_train.shape[0]))\n",
    "t0 = time()\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized',\n",
    "          whiten=True).fit(X_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projecting the input data on the eigenfaces orthonormal basis\n",
      "done in 4.787s\n"
     ]
    }
   ],
   "source": [
    "print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
    "t0 = time()\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier to the training set\n",
      "done in 0.613s\n",
      "Best estimator found by grid search:\n",
      "SVC(C=1000.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.0001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "print(\"Fitting the classifier to the training set\")\n",
    "t0 = time()\n",
    "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)\n",
    "clf = clf.fit(X_train_pca, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting people's names on the test set\n",
      "done in 0.001s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "n_classes = 2\n",
    "\n",
    "print(\"Predicting people's names on the test set\")\n",
    "t0 = time()\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        aia       1.00      1.00      1.00       315\n",
      "     bonnie       1.00      1.00      1.00       353\n",
      "\n",
      "avg / total       1.00      1.00      1.00       668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=trainNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bonnie', 'aia', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'aia', 'bonnie', 'aia', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'aia', 'bonnie', 'aia', 'aia', 'aia', 'bonnie', 'aia', 'aia',\n",
       "       'bonnie', 'bonnie', 'aia', 'aia', 'bonnie', 'bonnie', 'aia',\n",
       "       'bonnie', 'bonnie', 'aia', 'bonnie', 'aia', 'bonnie', 'aia', 'aia',\n",
       "       'aia', 'aia', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'aia', 'aia', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'aia', 'bonnie', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'aia', 'aia', 'bonnie', 'bonnie', 'aia', 'bonnie', 'bonnie', 'aia',\n",
       "       'aia', 'bonnie', 'aia', 'aia', 'aia', 'aia', 'aia', 'bonnie', 'aia',\n",
       "       'aia', 'aia', 'bonnie', 'aia', 'aia', 'bonnie', 'aia', 'aia', 'aia',\n",
       "       'bonnie', 'bonnie', 'aia', 'aia', 'aia', 'aia', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'aia', 'bonnie', 'bonnie', 'aia', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'aia', 'bonnie', 'aia', 'aia',\n",
       "       'bonnie', 'bonnie', 'aia', 'bonnie', 'bonnie', 'aia', 'bonnie',\n",
       "       'aia', 'aia', 'bonnie', 'bonnie', 'aia', 'bonnie', 'aia', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'aia', 'aia', 'aia', 'bonnie', 'aia', 'aia',\n",
       "       'aia', 'aia', 'bonnie', 'bonnie', 'aia', 'bonnie', 'aia', 'bonnie',\n",
       "       'bonnie', 'aia', 'bonnie', 'aia', 'bonnie', 'bonnie', 'aia', 'aia',\n",
       "       'aia', 'aia', 'bonnie', 'bonnie', 'aia', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'aia', 'aia', 'aia', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'aia', 'aia', 'aia', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'aia', 'aia', 'aia', 'bonnie', 'aia', 'aia', 'bonnie',\n",
       "       'aia', 'aia', 'aia', 'bonnie', 'aia', 'aia', 'aia', 'aia', 'bonnie',\n",
       "       'bonnie', 'aia', 'aia', 'aia', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'aia', 'bonnie', 'bonnie', 'aia', 'aia',\n",
       "       'bonnie', 'aia', 'bonnie', 'aia', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'aia', 'bonnie', 'aia', 'aia', 'bonnie', 'aia', 'aia',\n",
       "       'aia', 'bonnie', 'bonnie', 'aia', 'bonnie', 'aia', 'aia', 'bonnie',\n",
       "       'aia', 'bonnie', 'aia', 'aia', 'aia', 'aia', 'bonnie', 'bonnie',\n",
       "       'aia', 'aia', 'bonnie', 'aia', 'bonnie', 'aia', 'aia', 'bonnie',\n",
       "       'bonnie', 'aia', 'aia', 'aia', 'aia', 'bonnie', 'bonnie', 'aia',\n",
       "       'aia', 'aia', 'aia', 'bonnie', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'bonnie', 'aia', 'bonnie', 'bonnie',\n",
       "       'aia', 'aia', 'bonnie', 'bonnie', 'aia', 'aia', 'bonnie', 'aia',\n",
       "       'bonnie', 'aia', 'bonnie', 'aia', 'aia', 'bonnie', 'bonnie', 'aia',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'bonnie', 'aia', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'aia', 'bonnie', 'bonnie', 'aia', 'bonnie', 'aia',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'aia',\n",
       "       'aia', 'aia', 'aia', 'aia', 'aia', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'aia', 'bonnie', 'bonnie', 'aia', 'aia', 'bonnie', 'aia',\n",
       "       'aia', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'aia', 'aia',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'aia', 'aia',\n",
       "       'bonnie', 'aia', 'bonnie', 'bonnie', 'bonnie', 'aia', 'bonnie',\n",
       "       'bonnie', 'aia', 'bonnie', 'aia', 'aia', 'aia', 'aia', 'aia',\n",
       "       'bonnie', 'bonnie', 'aia', 'aia', 'aia', 'aia', 'bonnie', 'bonnie',\n",
       "       'aia', 'bonnie', 'aia', 'aia', 'bonnie', 'bonnie', 'bonnie', 'aia',\n",
       "       'bonnie', 'aia', 'bonnie', 'aia', 'aia', 'bonnie', 'bonnie', 'aia',\n",
       "       'bonnie', 'aia', 'aia', 'aia', 'aia', 'aia', 'bonnie', 'aia', 'aia',\n",
       "       'aia', 'aia', 'aia', 'bonnie', 'bonnie', 'bonnie', 'aia', 'bonnie',\n",
       "       'bonnie', 'aia', 'aia', 'bonnie', 'aia', 'bonnie', 'aia', 'aia',\n",
       "       'aia', 'bonnie', 'aia', 'aia', 'aia', 'aia', 'aia', 'aia', 'aia',\n",
       "       'bonnie', 'aia', 'bonnie', 'bonnie', 'aia', 'bonnie', 'bonnie',\n",
       "       'aia', 'bonnie', 'aia', 'bonnie', 'bonnie', 'bonnie', 'aia',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'aia', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'aia', 'bonnie', 'aia', 'bonnie', 'aia', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'aia', 'aia', 'bonnie', 'bonnie', 'bonnie', 'aia',\n",
       "       'bonnie', 'aia', 'aia', 'bonnie', 'bonnie', 'aia', 'bonnie', 'aia',\n",
       "       'aia', 'bonnie', 'aia', 'aia', 'bonnie', 'aia', 'bonnie', 'aia',\n",
       "       'aia', 'bonnie', 'aia', 'aia', 'aia', 'aia', 'aia', 'aia', 'aia',\n",
       "       'bonnie', 'aia', 'bonnie', 'aia', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'aia', 'aia', 'bonnie', 'bonnie', 'bonnie', 'aia', 'aia',\n",
       "       'aia', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'aia',\n",
       "       'bonnie', 'aia', 'aia', 'bonnie', 'aia', 'bonnie', 'aia', 'bonnie',\n",
       "       'bonnie', 'aia', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'aia',\n",
       "       'aia', 'aia', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'aia', 'bonnie', 'bonnie', 'aia',\n",
       "       'aia', 'bonnie', 'aia', 'bonnie', 'aia', 'aia', 'aia', 'bonnie',\n",
       "       'aia', 'bonnie', 'aia', 'aia', 'bonnie', 'bonnie', 'aia', 'bonnie',\n",
       "       'bonnie', 'aia', 'aia', 'aia', 'aia', 'bonnie', 'bonnie', 'aia',\n",
       "       'bonnie', 'aia', 'bonnie', 'aia', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'aia', 'aia', 'aia', 'aia', 'aia', 'bonnie', 'aia',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'aia', 'bonnie', 'aia', 'aia', 'aia',\n",
       "       'bonnie', 'aia', 'aia', 'aia', 'aia', 'bonnie', 'bonnie', 'aia',\n",
       "       'bonnie', 'bonnie', 'aia', 'bonnie', 'aia', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'aia', 'bonnie', 'aia', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'aia', 'bonnie', 'aia', 'bonnie',\n",
       "       'aia', 'bonnie', 'aia', 'aia', 'bonnie', 'aia', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'aia',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'aia', 'aia', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'aia',\n",
       "       'bonnie', 'aia', 'aia', 'aia', 'bonnie', 'aia', 'aia', 'aia',\n",
       "       'bonnie', 'aia', 'bonnie', 'aia', 'aia', 'aia', 'aia', 'aia',\n",
       "       'bonnie', 'aia', 'bonnie', 'bonnie', 'aia', 'bonnie', 'aia', 'aia',\n",
       "       'aia', 'aia', 'aia', 'aia', 'bonnie', 'aia', 'aia', 'aia', 'aia',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'aia', 'bonnie', 'aia', 'aia',\n",
       "       'bonnie', 'bonnie'], \n",
       "      dtype='|S6')"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bonnie', 'aia', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'aia', 'bonnie', 'aia', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'aia', 'bonnie', 'aia', 'aia', 'aia', 'bonnie', 'aia', 'aia',\n",
       "       'bonnie', 'bonnie', 'aia', 'aia', 'bonnie', 'bonnie', 'aia',\n",
       "       'bonnie', 'bonnie', 'aia', 'bonnie', 'aia', 'bonnie', 'aia', 'aia',\n",
       "       'aia', 'aia', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'aia', 'aia', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'aia', 'bonnie', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'aia', 'aia', 'bonnie', 'bonnie', 'aia', 'bonnie', 'bonnie', 'aia',\n",
       "       'aia', 'bonnie', 'aia', 'aia', 'aia', 'aia', 'aia', 'bonnie', 'aia',\n",
       "       'aia', 'aia', 'bonnie', 'aia', 'aia', 'bonnie', 'aia', 'aia', 'aia',\n",
       "       'bonnie', 'bonnie', 'aia', 'aia', 'aia', 'aia', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'aia', 'bonnie', 'bonnie', 'aia', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'aia', 'bonnie', 'aia', 'aia',\n",
       "       'bonnie', 'bonnie', 'aia', 'bonnie', 'bonnie', 'aia', 'bonnie',\n",
       "       'aia', 'aia', 'bonnie', 'bonnie', 'aia', 'bonnie', 'aia', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'aia', 'aia', 'aia', 'bonnie', 'aia', 'aia',\n",
       "       'aia', 'aia', 'bonnie', 'bonnie', 'aia', 'bonnie', 'aia', 'bonnie',\n",
       "       'bonnie', 'aia', 'bonnie', 'aia', 'bonnie', 'bonnie', 'aia', 'aia',\n",
       "       'aia', 'aia', 'bonnie', 'bonnie', 'aia', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'aia', 'aia', 'aia', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'aia', 'aia', 'aia', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'aia', 'aia', 'aia', 'bonnie', 'aia', 'aia', 'bonnie',\n",
       "       'aia', 'aia', 'aia', 'bonnie', 'aia', 'aia', 'aia', 'aia', 'bonnie',\n",
       "       'bonnie', 'aia', 'aia', 'aia', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'aia', 'bonnie', 'bonnie', 'aia', 'aia',\n",
       "       'bonnie', 'aia', 'bonnie', 'aia', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'aia', 'bonnie', 'aia', 'aia', 'bonnie', 'aia', 'aia',\n",
       "       'aia', 'bonnie', 'bonnie', 'aia', 'bonnie', 'aia', 'aia', 'bonnie',\n",
       "       'aia', 'bonnie', 'aia', 'aia', 'aia', 'aia', 'bonnie', 'bonnie',\n",
       "       'aia', 'aia', 'bonnie', 'aia', 'bonnie', 'aia', 'aia', 'bonnie',\n",
       "       'bonnie', 'aia', 'aia', 'aia', 'aia', 'bonnie', 'bonnie', 'aia',\n",
       "       'aia', 'aia', 'aia', 'bonnie', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'bonnie', 'aia', 'bonnie', 'bonnie',\n",
       "       'aia', 'aia', 'bonnie', 'bonnie', 'aia', 'aia', 'bonnie', 'aia',\n",
       "       'bonnie', 'aia', 'bonnie', 'aia', 'aia', 'bonnie', 'bonnie', 'aia',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'bonnie', 'aia', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'aia', 'bonnie', 'bonnie', 'aia', 'bonnie', 'aia',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'aia',\n",
       "       'aia', 'aia', 'aia', 'aia', 'aia', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'aia', 'bonnie', 'bonnie', 'aia', 'aia', 'bonnie', 'aia',\n",
       "       'aia', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'aia', 'aia',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'aia', 'aia',\n",
       "       'bonnie', 'aia', 'bonnie', 'bonnie', 'bonnie', 'aia', 'bonnie',\n",
       "       'bonnie', 'aia', 'bonnie', 'aia', 'aia', 'aia', 'aia', 'aia',\n",
       "       'bonnie', 'bonnie', 'aia', 'aia', 'aia', 'aia', 'bonnie', 'bonnie',\n",
       "       'aia', 'bonnie', 'aia', 'aia', 'bonnie', 'bonnie', 'bonnie', 'aia',\n",
       "       'bonnie', 'aia', 'bonnie', 'aia', 'aia', 'bonnie', 'bonnie', 'aia',\n",
       "       'bonnie', 'aia', 'aia', 'aia', 'aia', 'aia', 'bonnie', 'aia', 'aia',\n",
       "       'aia', 'aia', 'aia', 'bonnie', 'bonnie', 'bonnie', 'aia', 'bonnie',\n",
       "       'bonnie', 'aia', 'aia', 'bonnie', 'aia', 'bonnie', 'aia', 'aia',\n",
       "       'aia', 'bonnie', 'aia', 'aia', 'aia', 'aia', 'aia', 'aia', 'aia',\n",
       "       'bonnie', 'aia', 'bonnie', 'bonnie', 'aia', 'bonnie', 'bonnie',\n",
       "       'aia', 'bonnie', 'aia', 'bonnie', 'bonnie', 'bonnie', 'aia',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'aia', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'aia', 'bonnie', 'aia', 'bonnie', 'aia', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'aia', 'aia', 'bonnie', 'bonnie', 'bonnie', 'aia',\n",
       "       'bonnie', 'aia', 'aia', 'bonnie', 'bonnie', 'aia', 'bonnie', 'aia',\n",
       "       'aia', 'bonnie', 'aia', 'aia', 'bonnie', 'aia', 'bonnie', 'aia',\n",
       "       'aia', 'bonnie', 'aia', 'aia', 'aia', 'aia', 'aia', 'aia', 'aia',\n",
       "       'bonnie', 'aia', 'bonnie', 'aia', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'aia', 'aia', 'bonnie', 'bonnie', 'bonnie', 'aia', 'aia',\n",
       "       'aia', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'aia',\n",
       "       'bonnie', 'aia', 'aia', 'bonnie', 'aia', 'bonnie', 'aia', 'bonnie',\n",
       "       'bonnie', 'aia', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'aia',\n",
       "       'aia', 'aia', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'aia', 'bonnie', 'bonnie', 'aia',\n",
       "       'aia', 'bonnie', 'aia', 'bonnie', 'aia', 'aia', 'aia', 'bonnie',\n",
       "       'aia', 'bonnie', 'aia', 'aia', 'bonnie', 'bonnie', 'aia', 'bonnie',\n",
       "       'bonnie', 'aia', 'aia', 'aia', 'aia', 'bonnie', 'bonnie', 'aia',\n",
       "       'bonnie', 'aia', 'bonnie', 'aia', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'aia', 'aia', 'aia', 'aia', 'aia', 'bonnie', 'aia',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'aia', 'bonnie', 'aia', 'aia', 'aia',\n",
       "       'bonnie', 'aia', 'aia', 'aia', 'aia', 'bonnie', 'bonnie', 'aia',\n",
       "       'bonnie', 'bonnie', 'aia', 'bonnie', 'aia', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'aia', 'bonnie', 'aia', 'bonnie', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'aia', 'bonnie', 'aia', 'bonnie',\n",
       "       'aia', 'bonnie', 'aia', 'aia', 'bonnie', 'aia', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'aia',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'aia', 'aia', 'bonnie', 'bonnie',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'bonnie', 'aia',\n",
       "       'bonnie', 'aia', 'aia', 'aia', 'bonnie', 'aia', 'aia', 'aia',\n",
       "       'bonnie', 'aia', 'bonnie', 'aia', 'aia', 'aia', 'aia', 'aia',\n",
       "       'bonnie', 'aia', 'bonnie', 'bonnie', 'aia', 'bonnie', 'aia', 'aia',\n",
       "       'aia', 'aia', 'aia', 'aia', 'bonnie', 'aia', 'aia', 'aia', 'aia',\n",
       "       'bonnie', 'bonnie', 'bonnie', 'aia', 'bonnie', 'aia', 'aia',\n",
       "       'bonnie', 'bonnie'], \n",
       "      dtype='|S6')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Face defaults\n",
    "cascPath = '/usr/local/Cellar/opencv/2.4.13.2/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml'\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "minFaceSize = 100\n",
    "minNeighborVal = 5\n",
    "\n",
    "# Set paths, variable values\n",
    "faceDir = '/Users/abock/Desktop/InsightProject/FERG_DB_256/'\n",
    "# trainNames = ['aia','bonnie','jules','malcolm','mery']\n",
    "trainNames = ['aia','bonnie']\n",
    "testNames = ['ray']\n",
    "emotions = ['anger','disgust','fear','joy','neutral','sadness','surprise']\n",
    "# Loop through all images in the input directory\n",
    "testImages = []\n",
    "testSubs = []\n",
    "testEmotions = []\n",
    "for subName in testNames:\n",
    "    for emoName in emotions:\n",
    "        inDir = ''.join((faceDir,subName,'/',subName,'_',emoName,'/'))\n",
    "        for inFace in glob.glob(inDir + '*.png'):\n",
    "            # Load the image\n",
    "            rawImg = cv2.imread(inFace)\n",
    "            # Convert to gray\n",
    "            gImg = cv2.cvtColor(rawImg, cv2.COLOR_BGR2GRAY)\n",
    "            # Resize \n",
    "            img = cv2.resize(gImg,(outSize,outSize),interpolation = cv2.INTER_CUBIC)\n",
    "    \n",
    "            cv2.imshow('Face',img)\n",
    "            cv2.waitKey(1)\n",
    "            # Flatten\n",
    "            l = img.shape[0] * img.shape[1]\n",
    "            imgRow = img.reshape(1, l)\n",
    "            # Append to output matrices\n",
    "            testImages.append(imgRow[0])\n",
    "            testSubs.append(subName)\n",
    "            testEmotions.append(emoName)\n",
    "\n",
    "testImages = np.asarray(testImages)\n",
    "testSubs = np.asarray(testSubs)\n",
    "testEmotions = np.asarray(testEmotions)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def asRowMatrix (X):\n",
    "    if len (X) == 0:\n",
    "        return np . array ([])\n",
    "    mat = np . empty ((0 , X [0]. size ) , dtype = X [0]. dtype )\n",
    "    for row in X:\n",
    "        mat = np . vstack (( mat , np . asarray ( row ). reshape (1 , -1) ))\n",
    "        return mat\n",
    "\n",
    "def asColumnMatrix (X):\n",
    "    if len (X) == 0:\n",
    "        return np . array ([])\n",
    "    mat = np . empty (( X [0]. size , 0) , dtype = X [0]. dtype )\n",
    "    for col in X:\n",
    "        mat = np . hstack (( mat , np . asarray ( col ). reshape ( -1 ,1) ))\n",
    "        return mat\n",
    "    \n",
    "def pca (X , y , num_components):\n",
    "    [n , d] = X.shape\n",
    "    if (num_components <= 0) or (num_components > n):\n",
    "        num_components = n\n",
    "        mu = X.mean (axis = 0)\n",
    "        X = X - mu\n",
    "    if n > d:\n",
    "        C = np.dot(X.T,X)\n",
    "        [eigenvalues, eigenvectors] = np . linalg . eigh (C)\n",
    "    else:\n",
    "        C = np.dot (X ,X .T)\n",
    "        [eigenvalues, eigenvectors] = np.linalg.eigh(C)\n",
    "        eigenvectors = np.dot(X.T, eigenvectors)\n",
    "        for i in xrange (n):\n",
    "            eigenvectors [:, i] = eigenvectors [:, i] / np.linalg.norm(eigenvectors[:, i])\n",
    "            # or simply perform an economy size decomposition\n",
    "            # eigenvectors , eigenvalues , variance = np. linalg . svd (X.T, full_matrices = False )\n",
    "            # sort eigenvectors descending by their eigenvalue\n",
    "    idx = np . argsort ( - eigenvalues )\n",
    "    eigenvalues = eigenvalues [ idx ]\n",
    "    eigenvectors = eigenvectors [: , idx ]\n",
    "    # select only num_components\n",
    "    eigenvalues = eigenvalues [0: num_components ]. copy ()\n",
    "    eigenvectors = eigenvectors [: ,0: num_components ]. copy ()\n",
    "    return [ eigenvalues , eigenvectors , mu ]\n",
    "\n",
    "def project (W , X , mu = None ):\n",
    "    if mu is None :\n",
    "        return np . dot (X ,W)\n",
    "    return np . dot (X - mu , W)\n",
    "\n",
    "def reconstruct (W , Y , mu = None ) :\n",
    "    if mu is None :\n",
    "        return np . dot (Y ,W.T)\n",
    "    return np . dot (Y , W .T) + mu\n",
    "\n",
    "def normalize (X , low , high , dtype = None ):\n",
    "    X = np . asarray (X)\n",
    "    minX , maxX = np . min (X ) , np . max (X)\n",
    "    # normalize to [0...1].\n",
    "    X = X - float ( minX )\n",
    "    X = X / float (( maxX - minX ) )\n",
    "    # scale to [ low ... high ].\n",
    "    X = X * ( high - low )\n",
    "    X = X + low\n",
    "    if dtype is None :\n",
    "        return np . asarray (X)\n",
    "    return np . asarray (X , dtype = dtype )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time \n",
    "\n",
    "n_components = 2\n",
    "\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\"\n",
    "      % (n_components, X_train.shape[0]))\n",
    "t0 = time()\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized',\n",
    "          whiten=True).fit(trainImages)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "train_x = pca.fit(allImages)\n",
    "test_x = pca.transform(testImages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "from sklearn import cross_val, datasets, decomposition, svm\n",
    "\n",
    "# ..\n",
    "# .. load data ..\n",
    "lfw_people = datasets.fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "perm = np.random.permutation(lfw_people.target.size)\n",
    "lfw_people.data = lfw_people.data[perm]\n",
    "lfw_people.target = lfw_people.target[perm]\n",
    "faces = np.reshape(lfw_people.data, (lfw_people.target.shape[0], -1))\n",
    "train, test = iter(cross_val.StratifiedKFold(lfw_people.target, k=4)).next()\n",
    "X_train, X_test = faces[train], faces[test]\n",
    "y_train, y_test = lfw_people.target[train], lfw_people.target[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show cropped face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy\n",
    "import time\n",
    "inDir = '/Users/abock/Desktop/InsightProject/'\n",
    "cascPath = '/usr/local/Cellar/opencv/2.4.13.2/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml'\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "rawImg = cv2.imread(inDir + 'happy-person.png')\n",
    "h,w = rawImg.shape[:2]\n",
    "#imS = cv2.resize(rawImg, (w*2,h*2),interpolation = cv2.INTER_CUBIC)\n",
    "imS = rawImg\n",
    "gray = cv2.cvtColor(imS, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect faces in the image\n",
    "faces = faceCascade.detectMultiScale(\n",
    "    imS,\n",
    "    scaleFactor=1.1,\n",
    "    minNeighbors=1,\n",
    "    minSize=(200, 200),\n",
    "    flags = cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(imS, (x, y), (x+w, y+h), (0, 255, 0), 4)\n",
    "    face = imS[y:y+h,x:x+w]\n",
    "    cv2.imshow('Face',face)\n",
    "    cv2.waitKey(30)\n",
    "\n",
    "time.sleep(3)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy\n",
    "import glob\n",
    "\n",
    "inDir = '/Users/abock/Desktop/InsightProject/FERG_DB_256/aia/aia_joy/'\n",
    "cascPath = '/usr/local/Cellar/opencv/2.4.13.2/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml'\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "allFiles = glob.glob(inDir + '*png')\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "outFile = '/Users/abock/Desktop/aia_joy.avi'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "\n",
    "vid = None\n",
    "is_color = True\n",
    "ct = 0\n",
    "for inFace in allFiles:\n",
    "    ct = ct + 1\n",
    "    rawImg = cv2.imread(inFace)\n",
    "    h,w = rawImg.shape[:2]\n",
    "    #imS = cv2.resize(rawImg, (w*3,h*3),interpolation = cv2.INTER_CUBIC)\n",
    "    imS = rawImg\n",
    "    gray = cv2.cvtColor(imS, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    if vid is None:\n",
    "        vid = cv2.VideoWriter(outFile,fourcc, 30.0, (256,256))\n",
    "\n",
    "    # Detect faces in the image\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=1,\n",
    "        minSize=(150, 150),\n",
    "        flags = cv2.CASCADE_SCALE_IMAGE\n",
    "        )\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(gray, (x, y), (x+w, y+h), (0, 255, 0), 4)\n",
    "        vid.write(gray)\n",
    "        cv2.imshow('Face',gray)\n",
    "        cv2.waitKey(30)\n",
    "    if ct > 10:\n",
    "        break\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import mean,cov,cumsum,dot,linalg,size,flipud,argsort\n",
    "\n",
    "def princomp(A,numpc=0):\n",
    "    # computing eigenvalues and eigenvectors of covariance matrix\n",
    "    M = (A-mean(A.T,axis=1)).T # subtract the mean (along columns)\n",
    "    [latent,coeff] = linalg.eig(cov(M))\n",
    "    p = size(coeff,axis=1)\n",
    "    idx = argsort(latent) # sorting the eigenvalues\n",
    "    idx = idx[::-1]       # in ascending order\n",
    "    # sorting eigenvectors according to the sorted eigenvalues\n",
    "    coeff = coeff[:,idx]\n",
    "    latent = latent[idx] # sorting eigenvalues\n",
    "    if numpc < p and numpc >= 0:\n",
    "            coeff = coeff[:,range(numpc)] # cutting some PCs if needed\n",
    "    score = dot(coeff.T,M) # projection of the data in the new space\n",
    "    return coeff,score,latent\n",
    "\n",
    "from pylab import imread,subplot,imshow,title,gray,figure,show,NullLocator\n",
    "A = rawImg\n",
    "A = mean(A,2) # to get a 2-D array\n",
    "full_pc = size(A,axis=1) # numbers of all the principal components\n",
    "i = 1\n",
    "dist = []\n",
    "\n",
    "step = 10\n",
    "for numpc in range(0,full_pc+step,step): # 0 step*1 step*2 ... full_pc\n",
    "    coeff, score, latent = princomp(A,numpc)\n",
    "    Ar = dot(coeff,score).T+mean(A,axis=0) # image reconstruction\n",
    "    # difference in Frobenius norm\n",
    "    dist.append(linalg.norm(A-Ar,'fro'))\n",
    "    # showing the pics reconstructed with less than 50 PCs\n",
    "    if numpc <= 50:\n",
    "        ax = subplot(2,3,i,frame_on=False)\n",
    "        ax.xaxis.set_major_locator(NullLocator()) # remove ticks\n",
    "        ax.yaxis.set_major_locator(NullLocator())\n",
    "        i += 1 \n",
    "        imshow(Ar)\n",
    "        title('PCs # '+str(numpc))\n",
    "        gray()\n",
    "\n",
    "figure()\n",
    "imshow(A)\n",
    "title('numpc FULL')\n",
    "gray()\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy\n",
    "import time\n",
    "inDir = '/Users/abock/Desktop/faceMovie/'\n",
    "\n",
    "# Detect faces in the image\n",
    "cascPath = '/usr/local/Cellar/opencv/2.4.13.2/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml'\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "for i in range(1,11):\n",
    "    rawImg = cv2.imread(inDir + 'frame' + str(i) + '.jpeg')\n",
    "    imS = rawImg\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "        imS,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=1,\n",
    "        minSize=(100, 100),\n",
    "        flags = cv2.CASCADE_SCALE_IMAGE\n",
    "        )\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(imS, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        cv2.imshow('Face',imS)\n",
    "        cv2.waitKey(30)\n",
    "time.sleep(3)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawImg = cv2.imread('/Users/abock/aia_anger_561.png')\n",
    "imS = rawImg\n",
    "gray = cv2.cvtColor(imS, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "\n",
    "faces = faceCascade.detectMultiScale(\n",
    "    gray,\n",
    "    scaleFactor=1.1,\n",
    "    minNeighbors=1,\n",
    "    minSize=(10, 10),\n",
    "    flags = cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(gray, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    cv2.imshow('gray',gray)\n",
    "    cv2.waitKey(30)\n",
    "    \n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cascPath = '/usr/local/Cellar/opencv/2.4.13.2/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml'\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "rawImg = cv2.imread('/Users/abock/aia_anger_561.png')\n",
    "imS = rawImg\n",
    "gray = cv2.cvtColor(imS, cv2.COLOR_BGR2GRAY)\n",
    "faces = faceCascade.detectMultiScale(\n",
    "    gray,\n",
    "    scaleFactor=1.1,\n",
    "    minNeighbors=1,\n",
    "    minSize=(10, 10),\n",
    "    flags = cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(imS, (x, y), (x+w, y+h), (0, 255, 0), 4)\n",
    "    cv2.imshow('Face',imS)\n",
    "    cv2.waitKey(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record Movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "\n",
    "outDir = '/Users/abock/Desktop/faceMovie/'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "ct = 0\n",
    "while ct < 50:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',gray)\n",
    "    cv2.waitKey(30)\n",
    "    \n",
    "    # save the frames\n",
    "    ct = ct + 1\n",
    "    outFrame = os.path.join(outDir, \"frame\" + str(ct) + \".jpeg\")\n",
    "    cv2.imwrite(outFrame,gray)\n",
    "    \n",
    "    # save a movie\n",
    "    cv2.write(outVid,gray)\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "Y = iris.target\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "logreg.fit(X, Y)\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
