{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# faceRec\n",
    "\n",
    "This IPython notebook will recognize a face in a video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cohn-Kanade AU-Coded Expression Database - CK+\n",
    " \n",
    "123 subjects (85 female)\n",
    "\n",
    "emotional expressions were scored/validated, so there are a variable number of emotions / subject \n",
    "\n",
    "happy, sad, surprise, neutral, anger, disgust, fear \n",
    "\n",
    "\n",
    "CK+ Notes:\n",
    "\n",
    "The Emotion coded files (Emotion_labels.zip) - ONLY 327 of the 593 sequences have emotion sequences. This is because these are the only ones the fit the prototypic definition. Like the FACS files, there is only 1 Emotion file for each sequence which is the last frame (the peak frame). There should be only one entry and the number will range from 0-7 (i.e. 0=neutral, 1=anger, 2=contempt, 3=disgust, 4=fear, 5=happy, 6=sadness, 7=surprise). N.B there is only 327 files- IF THERE IS NO FILE IT MEANS THAT THERE IS NO EMOTION LABEL (sorry to be explicit but this will avoid confusion)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the faces and emotion files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from time import time\n",
    "import cPickle as pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import neighbors\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Initialize variables\n",
    "subjectNames = []\n",
    "subjectFiles = []\n",
    "subjectEmotions = []\n",
    "subjectImages = []\n",
    "emotions = ['anger','disgust','fear','joy','neutral','sadness','surprise']\n",
    "emotionLabels = ['anger','disgust','joy','sadness','surprise']\n",
    "\n",
    "# OpenCV face classifier\n",
    "cascPath = '/usr/local/Cellar/opencv/2.4.13.2/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml'\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "# Face processing defaults\n",
    "outImgSize = 500\n",
    "scaleSize = 1.05\n",
    "numNeighbors = 3\n",
    "faceSize = 150\n",
    "\n",
    "# Classifier\n",
    "n_components = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CK+ images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input images...\n",
      "done in 20.520s\n"
     ]
    }
   ],
   "source": [
    "# Set paths, variable values\n",
    "dbDir = '/Users/abock/Desktop/InsightProject/faceDatabases/CK+/'\n",
    "emoDir = '/Users/abock/Desktop/InsightProject/faceDatabases/CK+/Emotion/'\n",
    "emoNums = [1,3,4,5,0,6,7]\n",
    "\n",
    "print(\"Loading input images...\")\n",
    "t0 = time()\n",
    "\n",
    "# Find the images and emotion labels\n",
    "for subDir in glob.glob(emoDir + 'S*/'):\n",
    "    subject = os.path.basename(os.path.dirname(subDir))\n",
    "    # Append to subjectEmotions\n",
    "    subjectNames.append(subject)\n",
    "    for faceDir in glob.glob(subDir + '*/'):\n",
    "        dirNum = os.path.basename(os.path.dirname(faceDir))\n",
    "        for thisFile in glob.glob(faceDir + '*.txt'):\n",
    "            imDir = os.path.join(dbDir,subject,dirNum)\n",
    "            fName = os.path.basename(thisFile[:-12])\n",
    "            imFile = os.path.join(imDir,fName + '.png')\n",
    "            \n",
    "            with open(thisFile) as f:\n",
    "                for line in f:\n",
    "                    line = map(float, line.split())\n",
    "                    lineInt = int(line[0])\n",
    "#                     if lineInt != 2: # 2 is contempt\n",
    "                    if lineInt != 2 and lineInt !=4: # 2 is contempt, 4 is fear\n",
    "                        thisEmotion = emotions[emoNums.index(int(line[0]))]\n",
    "            \n",
    "            # Load the image\n",
    "            rawImg = cv2.imread(imFile)\n",
    "\n",
    "            # Detect faces in the image\n",
    "            faces = faceCascade.detectMultiScale(\n",
    "            rawImg,\n",
    "            scaleFactor=scaleSize,\n",
    "            minNeighbors=numNeighbors,\n",
    "            minSize=(faceSize, faceSize),\n",
    "            flags = cv2.CASCADE_SCALE_IMAGE\n",
    "            )\n",
    "\n",
    "            # If a face is detected\n",
    "            if len(faces):\n",
    "                # Convert to gray\n",
    "                gImg = cv2.cvtColor(rawImg, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # Resize just the face\n",
    "                x,y,w,h = faces[0,:]\n",
    "                faceImg = gImg[y:y+h,x:x+w]\n",
    "                img = cv2.resize(faceImg,(outImgSize,outImgSize),interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "#                 cv2.imshow('Image',img)\n",
    "#                 cv2.waitKey(30)\n",
    "\n",
    "                # Flatten\n",
    "                l = img.shape[0] * img.shape[1]\n",
    "                img = img.reshape(1, l)\n",
    "                \n",
    "                # Append to subjectNames\n",
    "                subjectNames.append(subject)\n",
    "                # Append to subjectEmotions\n",
    "                subjectEmotions.append(thisEmotion)\n",
    "                # Append to subjectFiles\n",
    "                subjectFiles.append(imFile) \n",
    "                # Append to output matrices\n",
    "                subjectImages.append(img[0])  \n",
    "                \n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to numpy array    \n",
    "subjectNames = np.asarray(subjectNames)\n",
    "subjectEmotions = np.asarray(subjectEmotions)\n",
    "subjectFiles = np.asarray(subjectFiles)\n",
    "subjectImages = np.asarray(subjectImages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the top 50 eigenfaces from 327 faces\n",
      "done in 8.391s\n",
      "Projecting the input data on the eigenfaces orthonormal basis\n",
      "done in 0.781s\n",
      "Fitting the classifier to the training set\n",
      "done in 3.571s\n",
      "Best estimator found by grid search:\n",
      "SVC(C=1000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "#     subjectImages, subjectEmotions, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = subjectImages\n",
    "Y_train = subjectEmotions\n",
    "\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\"\n",
    "      % (n_components, X_train.shape[0]))\n",
    "t0 = time()\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized',\n",
    "          whiten=True).fit(X_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
    "t0 = time()\n",
    "X_train_pca = pca.transform(X_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "print(\"Fitting the classifier to the training set\")\n",
    "t0 = time()\n",
    "\n",
    "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "\n",
    "#clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced',probability=True), param_grid)\n",
    "clf = GridSearchCV(SVC(kernel='rbf',probability=True), param_grid)\n",
    "\n",
    "clf = clf.fit(X_train_pca, Y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger\n",
      "56\n",
      "disgust\n",
      "62\n",
      "joy\n",
      "73\n",
      "sadness\n",
      "45\n",
      "surprise\n",
      "91\n"
     ]
    }
   ],
   "source": [
    "for emotion in emotionLabels:\n",
    "    print(emotion)\n",
    "    foo = np.array(subjectEmotions == emotion)\n",
    "    print(np.sum(foo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the training set to file\n",
      "done in 7.213s\n"
     ]
    }
   ],
   "source": [
    "# Save out the pca and classifier\n",
    "print(\"Saving the training set to file\")\n",
    "t0 = time()\n",
    "pickle.dump([pca,clf],open('save.p','w'))\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import cPickle as pickle\n",
    "# pca, clf = pickle.load(open('save.p','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the top 50 eigenfaces from 261 faces\n",
      "done in 7.340s\n",
      "Projecting the input data on the eigenfaces orthonormal basis\n",
      "done in 0.623s\n",
      "Fitting the classifier to the training set\n",
      "done in 2.652s\n",
      "Best estimator found by grid search:\n",
      "SVC(C=1000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    subjectImages, subjectEmotions, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\"\n",
    "      % (n_components, X_train.shape[0]))\n",
    "t0 = time()\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized',\n",
    "          whiten=True).fit(X_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
    "t0 = time()\n",
    "X_train_pca = pca.transform(X_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "print(\"Fitting the classifier to the training set\")\n",
    "t0 = time()\n",
    "\n",
    "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "\n",
    "#clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced',probability=True), param_grid)\n",
    "clf = GridSearchCV(SVC(kernel='rbf',probability=True), param_grid)\n",
    "\n",
    "# metrics = ['minkowski','euclidean','manhattan'] \n",
    "# weights = ['uniform','distance'] #10.0**np.arange(-5,4)\n",
    "# NN = np.arange(1,30)\n",
    "# #algorithms = ['ball_tree', 'kd_tree', 'brute']\n",
    "# algorithms = ['brute']\n",
    "# param_grid = dict(metric=metrics,weights=weights,n_neighbors=NN,algorithm=algorithms)\n",
    "# clf = GridSearchCV(neighbors.KNeighborsClassifier(), param_grid)\n",
    "\n",
    "clf = clf.fit(X_train_pca, Y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      anger       0.55      0.79      0.65        14\n",
      "    disgust       0.91      0.62      0.74        16\n",
      "        joy       1.00      1.00      1.00        12\n",
      "    sadness       0.67      0.57      0.62        14\n",
      "   surprise       0.73      0.80      0.76        10\n",
      "\n",
      "avg / total       0.77      0.74      0.74        66\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test_pca = pca.transform(X_test)\n",
    "Y_pred = clf.predict(X_test_pca)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Novel image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#inFace = '/Users/abock/Desktop/InsightProject/sad_keanu_Big.png'\n",
    "#inFace = '/Users/abock/Desktop/InsightProject/happy_keanu.png'\n",
    "inFace = '/Users/abock/Desktop/InsightProject/crazy-guy.png'\n",
    "#inFace = '/Users/abock/Desktop/InsightProject/happy_guy.png'\n",
    "#inFace = '/Users/abock/Desktop/InsightProject/happy_girl.png'\n",
    "#inFace = '/Users/abock/Desktop/InsightProject/smiling-women.jpg'\n",
    "#inFace = '/Users/abock/Desktop/InsightProject/sadGoslingMeme.jpg'\n",
    "\n",
    "#inFace = '/Users/abock/Desktop/InsightProject/aia_sadness_547.png'\n",
    "# inFace = '/Users/abock/Desktop/InsightProject/aia_anger_93.png'\n",
    "#inFace = '/Users/abock/Desktop/InsightProject/malcolm_joy_104.png'\n",
    "\n",
    "# Load the image\n",
    "rawImg = cv2.imread(inFace)\n",
    "\n",
    "#img = rawImg\n",
    "\n",
    "img = cv2.resize(rawImg,(500,500),interpolation = cv2.INTER_CUBIC) \n",
    "\n",
    "cv2.imshow('Image',img)\n",
    "cv2.waitKey(30)\n",
    "\n",
    "# Detect faces in the image\n",
    "faces = faceCascade.detectMultiScale(\n",
    "    img,\n",
    "    scaleFactor=scaleSize,\n",
    "    minNeighbors=1,\n",
    "    minSize=(10, 10),\n",
    "    flags = cv2.CASCADE_SCALE_IMAGE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the largest face\n",
    "i=np.where(faces[:,2] == faces[:,2].max())\n",
    "faces = faces[i,:]\n",
    "faces = faces[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If a face is detected\n",
    "if len(faces):\n",
    "    # Convert to gray\n",
    "    gImg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Resize just the face\n",
    "    x,y,w,h = faces[0,:]\n",
    "    faceImg = gImg[y:y+h,x:x+w]\n",
    "    img = cv2.resize(faceImg,(outImgSize,outImgSize),interpolation = cv2.INTER_CUBIC)\n",
    "    \n",
    "    # Flatten\n",
    "    l = img.shape[0] * img.shape[1]\n",
    "    img = img.reshape(1, l)\n",
    "\n",
    "testFace = np.asarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv2.imshow('Image',faceImg)\n",
    "cv2.waitKey(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_pca = pca.transform(testFace)\n",
    "Y_pred = clf.predict(X_test_pca)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "probs = clf.predict_proba(X_test_pca)\n",
    "df = DataFrame(probs,columns = emotionLabels)\n",
    "ax = df.plot.bar()\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.set_ylabel('Probability')\n",
    "plt.savefig(\"out.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultsMat = []\n",
    "for i in range(0,probs.shape[1]): # ['anger', 'disgust', 'joy', 'sadness', 'surprise']\n",
    "    resultsMat.append([emotionLabels[i],repr(probs[0,i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "df = DataFrame(resultsMat,columns=['Emotion','Probability'])\n",
    "\n",
    "print df.to_string(index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.plot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record Movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "\n",
    "outDir = '/Users/abock/Desktop/faceMovie/'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "cap = cv2.VideoCapture(0)\n",
    "w = cap.get(cv2.CAP_PROP_FRAME_WIDTH);\n",
    "h = cap.get(cv2.CAP_PROP_FRAME_HEIGHT); \n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('output.mp4',fourcc, 15.0, (int(w),int(h)))\n",
    "\n",
    "ct = 0\n",
    "while ct < 50:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret==True:\n",
    "        #frame = cv2.flip(frame,0)\n",
    "\n",
    "        # write the flipped frame\n",
    "        out.write(frame)\n",
    "\n",
    "        cv2.imshow('frame',frame)\n",
    "        cv2.waitKey(30)\n",
    "\n",
    "    ct = ct + 1\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vidcap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save movie as frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "vidcap = cv2.VideoCapture('output.mp4')\n",
    "success,image = vidcap.read()\n",
    "count = 0\n",
    "success = True\n",
    "while success:\n",
    "  success,image = vidcap.read()\n",
    "  print 'Read a new frame: ', success\n",
    "  cv2.imwrite(\"frame%d.jpg\" % count, image)     # save frame as JPEG file\n",
    "  count += 1\n",
    "    \n",
    "vidcap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "Y = iris.target\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "logreg.fit(X, Y)\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2 \n",
    "import urllib\n",
    "import os\n",
    "\n",
    "def saveURLimages(url,outDir):\n",
    "    req = urllib2.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    html = urllib2.urlopen(req).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    # get images\n",
    "    images = [img for img in soup.findAll('img')]\n",
    "    print (str(len(images)) + \" images found.\")\n",
    "    print 'Downloading images to: ' + outDir\n",
    "    \n",
    "    # download images\n",
    "    image_links = [each.get('src') for each in images]\n",
    "    #print(image_links)\n",
    "    for each in image_links:\n",
    "        if not each is None:\n",
    "            if each[0:4] == 'http':\n",
    "                #print(each)\n",
    "                filename=each.split('/')[-1]\n",
    "                urllib.urlretrieve(each,os.path.join(outDir,filename + '.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save happy faces (Google Images)\n",
    "happyPerson = 'https://www.google.com/search?site=&tbm=isch&source=hp&biw=1680&bih=949&q=happy+person&oq=happy+person&gs_l=img.3..0l10.15311.17067.0.17213.12.7.0.5.5.0.82.450.7.7.0....0...1ac.1.64.img..0.12.461.FpA8-IvoCOk'\n",
    "url = happyPerson\n",
    "outDir = '/Users/abock/Desktop/HappyFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "saveURLimages(url,outDir)\n",
    "\n",
    "# Save sad faces (Google Images)\n",
    "sadPerson = 'https://www.google.com/search?site=&tbm=isch&source=hp&biw=840&bih=949&q=sad+person&oq=sad+person&gs_l=img.3..0l10.732.2268.0.2413.10.7.0.3.3.0.337.1010.0j1j2j1.4.0....0...1ac.1.64.img..3.7.1015.rkrdqlI1p-k'\n",
    "url = sadPerson\n",
    "outDir = '/Users/abock/Desktop/SadFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "saveURLimages(url,outDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save happy faces (Bing Images)\n",
    "happyPerson = 'https://www.bing.com/images/search?q=happy%20person&qs=n&form=QBIR&pq=happy%20person&sc=8-12&sp=-1&sk='\n",
    "url = happyPerson\n",
    "outDir = '/Users/abock/Desktop/HappyFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "saveURLimages(url,outDir)\n",
    "\n",
    "# Save sad faces (Bing Images)\n",
    "sadPerson = 'https://www.bing.com/images/search?q=sad+person&qs=n&form=QBILPG&pq=sad+person&sc=8-8&sp=-1&sk='\n",
    "url = sadPerson\n",
    "outDir = '/Users/abock/Desktop/SadFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "saveURLimages(url,outDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Happy Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save happy faces (Yahoo Images)\n",
    "outDir = '/Users/abock/Desktop/HappyFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "    \n",
    "happyPerson = 'https://images.search.yahoo.com/search/images;_ylt=AwrTcXdqJYFYvE0A0juLuLkF?p=happy+person&ei=UTF-8&iscqry=&fr=sfp'\n",
    "url = happyPerson\n",
    "saveURLimages(url,outDir)\n",
    "\n",
    "happyHumanFace = 'https://images.search.yahoo.com/search/images;_ylt=AwrTcXyAJoFYWHMApB6JzbkF;_ylu=X3oDMTBsZ29xY3ZzBHNlYwNzZWFyY2gEc2xrA2J1dHRvbg--;_ylc=X1MDOTYwNjI4NTcEX3IDMgRhY3RuA2NsawRiY2sDNmltaTE4bGM2N29ybyUyNmIlM0QzJTI2cyUzRDI5BGNzcmNwdmlkAzNzeFdhVEl3Tmk1cFdrRkZXR1BqZUFkSE5qVXVPQUFBQUFDdHdqNncEZnIDc2ZwBGZyMgNzYS1ncARncHJpZANLQm5lbEt4X1JOeUlzNXY5WC40cVRBBG10ZXN0aWQDbnVsbARuX3N1Z2cDMTAEb3JpZ2luA2ltYWdlcy5zZWFyY2gueWFob28uY29tBHBvcwMwBHBxc3RyAwRwcXN0cmwDBHFzdHJsAzE2BHF1ZXJ5A2hhcHB5IGh1bWFuIGZhY2UEdF9zdG1wAzE0ODQ4NTkwNzYEdnRlc3RpZANudWxs?gprid=KBnelKx_RNyIs5v9X.4qTA&pvid=3sxWaTIwNi5pWkFFWGPjeAdHNjUuOAAAAACtwj6w&p=happy+human+face&fr=sfp&fr2=sb-top-images.search.yahoo.com&ei=UTF-8&n=60&x=wrt'\n",
    "url = happyHumanFace\n",
    "saveURLimages(url,outDir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sad Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output directory\n",
    "outDir = '/Users/abock/Desktop/SadFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "   \n",
    "# Yahoo Images\n",
    "sadPerson = 'https://images.search.yahoo.com/search/images;_ylt=AwrTcXZ6JYFYl68AvSGJzbkF;_ylu=X3oDMTBsZ29xY3ZzBHNlYwNzZWFyY2gEc2xrA2J1dHRvbg--;_ylc=X1MDOTYwNjI4NTcEX3IDMgRhY3RuA2NsawRiY2sDNmltaTE4bGM2N29ybyUyNmIlM0QzJTI2cyUzRDI5BGNzcmNwdmlkA3VBdHlfekl3Tmk1cFdrRkZXR1BqZUFDaE5qVXVPQUFBQUFDZUhiR3oEZnIDc2ZwBGZyMgNzYS1ncARncHJpZANnb3NOVG13V1RTdVc1MXA5ZmhMZGlBBG10ZXN0aWQDbnVsbARuX3N1Z2cDMTAEb3JpZ2luA2ltYWdlcy5zZWFyY2gueWFob28uY29tBHBvcwMwBHBxc3RyAwRwcXN0cmwDBHFzdHJsAzEwBHF1ZXJ5A3NhZCBwZXJzb24EdF9zdG1wAzE0ODQ4NTg3NjAEdnRlc3RpZANudWxs?gprid=gosNTmwWTSuW51p9fhLdiA&pvid=uAty_zIwNi5pWkFFWGPjeAChNjUuOAAAAACeHbGz&p=sad+person&fr=sfp&fr2=sb-top-images.search.yahoo.com&ei=UTF-8&n=60&x=wrt'\n",
    "url = sadPerson\n",
    "saveURLimages(url,outDir)\n",
    "\n",
    "sadHumanFace = 'https://images.search.yahoo.com/search/images;_ylt=AwrTcXmHJYFY_lwACdOJzbkF;_ylu=X3oDMTBsZ29xY3ZzBHNlYwNzZWFyY2gEc2xrA2J1dHRvbg--;_ylc=X1MDOTYwNjI4NTcEX3IDMgRhY3RuA2NsawRiY2sDNmltaTE4bGM2N29ybyUyNmIlM0QzJTI2cyUzRDI5BGNzcmNwdmlkA2N3Q2RJREl3Tmk1cFdrRkZXR1BqZUFUVU5qVXVPQUFBQUFDZTZzVXEEZnIDc2ZwBGZyMgNzYS1ncARncHJpZANKZHp4ZFBPMVFfSzFlYlZTTkpycGhBBG10ZXN0aWQDbnVsbARuX3N1Z2cDMTAEb3JpZ2luA2ltYWdlcy5zZWFyY2gueWFob28uY29tBHBvcwMwBHBxc3RyAwRwcXN0cmwDBHFzdHJsAzE0BHF1ZXJ5A3NhZCBodW1hbiBmYWNlBHRfc3RtcAMxNDg0ODU5MDA5BHZ0ZXN0aWQDbnVsbA--?gprid=JdzxdPO1Q_K1ebVSNJrphA&pvid=cwCdIDIwNi5pWkFFWGPjeATUNjUuOAAAAACe6sUq&p=sad+human+face&fr=sfp&fr2=sb-top-images.search.yahoo.com&ei=UTF-8&n=60&x=wrt'\n",
    "url = sadHumanFace\n",
    "saveURLimages(url,outDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Set paths, variable values\n",
    "# dbDir = '/Users/abock/Desktop/InsightProject/faceDatabases/FERG/'\n",
    "\n",
    "# print(\"Loading input images...\")\n",
    "# t0 = time()\n",
    "\n",
    "# # Find the images and emotion labels\n",
    "# for subDir in glob.glob(dbDir + '*/'):\n",
    "#     subject = os.path.basename(os.path.dirname(subDir))\n",
    "    \n",
    "#     for faceDir in glob.glob(subDir + '*/'):\n",
    "#         tmp = os.path.basename(os.path.dirname(faceDir))\n",
    "#         thisEmotion = tmp[len(subject)+1:]\n",
    "#         for imFile in glob.glob(faceDir + '*.png'):\n",
    "#             # Load the image\n",
    "#             rawImg = cv2.imread(imFile)\n",
    "\n",
    "#             # Detect faces in the image\n",
    "#             faces = faceCascade.detectMultiScale(\n",
    "#             rawImg,\n",
    "#             scaleFactor=scaleSize,\n",
    "#             minNeighbors=numNeighbors,\n",
    "#             minSize=(faceSize, faceSize),\n",
    "#             flags = cv2.CASCADE_SCALE_IMAGE\n",
    "#             )\n",
    "\n",
    "#             # If a face is detected\n",
    "#             if len(faces):\n",
    "#                 # Convert to gray\n",
    "#                 gImg = cv2.cvtColor(rawImg, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#                 # Resize just the face\n",
    "#                 x,y,w,h = faces[0,:]\n",
    "#                 faceImg = gImg[y:y+h,x:x+w]\n",
    "#                 img = cv2.resize(faceImg,(outImgSize,outImgSize),interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "# #                 cv2.imshow('Image',img)\n",
    "# #                 cv2.waitKey(30)\n",
    "\n",
    "#                 # Flatten\n",
    "#                 l = img.shape[0] * img.shape[1]\n",
    "#                 img = img.reshape(1, l)\n",
    "                \n",
    "#                 # Append to subjectEmotions\n",
    "#                 subjectNames.append(subject)\n",
    "#                 # Append to subjectEmotions\n",
    "#                 subjectEmotions.append(thisEmotion)\n",
    "#                 # Append to subjectFiles\n",
    "#                 subjectFiles.append(imFile) \n",
    "#                 # Append to output matrices\n",
    "#                 subjectImages.append(img[0])\n",
    "                \n",
    "# print(\"done in %0.3fs\" % (time() - t0))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
