{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# faceRec\n",
    "\n",
    "```\n",
    "classify facial expressions in images\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install packages\n",
    "\n",
    "    Run the following commands in terminal:\n",
    "\n",
    "```\n",
    "sudo apt-get install libopencv-dev python-opencv\n",
    "sudo apt-get install -y python-qt4\n",
    "sudo apt-get install python-matplotlib\n",
    "sudo apt-get install python-dill\n",
    "sudo pip install -U scikit-learn\n",
    "sudo apt-get install nginx\n",
    "sudo apt-get install gunicorn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details on face database\n",
    "\n",
    "```\n",
    "Cohn-Kanade AU-Coded Expression Database - CK+\n",
    "University of Pittsburgh, Pittsburgh, PA\n",
    "\n",
    "Lucey, P., Cohn, J. F., Kanade, T., Saragih, J., Ambadar, Z., & Matthews, I. (2010, June). The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE Computer Society Conference on (pp. 94-101). IEEE.\n",
    " \n",
    "123 subjects (85 female)\n",
    "\n",
    "FACS - Facial Action Coding System, extracts the geometrical features of the faces. Features are coded as action units (AU)\n",
    "\n",
    "To label an expression with one of the values below, it must contain the set of AUs associated with that emotion (see table 2 in paper)\n",
    "\n",
    "0=neutral, 1=anger, 2=contempt, 3=disgust, 4=fear, 5=happy, 6=sadness, 7=surprise\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from os.path import expanduser\n",
    "import numpy as np\n",
    "import cv2\n",
    "from time import time\n",
    "import cPickle as pickle\n",
    "import dill\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import neighbors, linear_model\n",
    "from sklearn.metrics import classification_report, accuracy_score, recall_score, precision_recall_curve, precision_score\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, precision_recall_fscore_support\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findFace(inDict):\n",
    "    faceCascade = inDict['faceCascade']\n",
    "    # Load the emotion image\n",
    "    rawImg = cv2.imread(inDict['imFile'])\n",
    "    \n",
    "    # Reshape\n",
    "    if rawImg.shape[0] > 800:\n",
    "        # Resize just the face\n",
    "        rawImg = cv2.resize(rawImg,(int(rawImg.shape[1]/10),int(rawImg.shape[0]/10)), interpolation=cv2.INTER_CUBIC)\n",
    " \n",
    "    # Detect faces in the image\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "        rawImg,\n",
    "        scaleFactor=inDict['scaleSize'],\n",
    "        minNeighbors=inDict['numNeighbors'],\n",
    "        minSize=(inDict['faceSize'], inDict['faceSize']),\n",
    "        flags = cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "    return rawImg, faces\n",
    "\n",
    "def findFlippedFace(inDict):\n",
    "    faceCascade = inDict['faceCascade']\n",
    "    # Load the emotion image\n",
    "    rawImg = cv2.imread(inDict['imFile'])\n",
    "    # Flip the image\n",
    "    rawImg=cv2.flip(rawImg,1)\n",
    "    \n",
    "    # Reshape\n",
    "    if rawImg.shape[0] > 500:\n",
    "        # Resize just the face\n",
    "        rawImg = cv2.resize(rawImg,(int(rawImg.shape[1]/10),int(rawImg.shape[0]/10)), interpolation=cv2.INTER_CUBIC)\n",
    " \n",
    "    # Detect faces in the image\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "        rawImg,\n",
    "        scaleFactor=inDict['scaleSize'],\n",
    "        minNeighbors=inDict['numNeighbors'],\n",
    "        minSize=(inDict['faceSize'], inDict['faceSize']),\n",
    "        flags = cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "    return rawImg, faces\n",
    "\n",
    "def face2vec(inDict):\n",
    "    faces = inDict['faces']\n",
    "    rawImg = inDict['rawImg']\n",
    "    outImgSize = inDict['outImgSize']\n",
    "    \n",
    "    if len(faces):\n",
    "        # Get the largest face\n",
    "        i = np.where(faces[:, 2] == faces[:, 2].max())\n",
    "        faces = faces[i, :]\n",
    "        faces = faces[0, :]\n",
    "        x, y, w, h = faces[0, :]\n",
    "        # Rectangle on face\n",
    "        cv2.rectangle(rawImg, (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
    "    \n",
    "        # Convert to gray\n",
    "        gImg = cv2.cvtColor(rawImg, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Resize just the face\n",
    "        x,y,w,h = faces[0,:]\n",
    "        faceImg = gImg[y:y+h,x:x+w]\n",
    "        faceImg = cv2.resize(faceImg,(outImgSize,outImgSize),interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "        # Show images\n",
    "        if inDict['showImage']:\n",
    "            cv2.imshow('Image',faceImg)\n",
    "            cv2.waitKey(30)\n",
    "\n",
    "        # Flatten\n",
    "        l = faceImg.shape[0] * faceImg.shape[1]\n",
    "        img = faceImg.reshape(1, l)\n",
    "    else:\n",
    "        img = []\n",
    "        faceImg = []\n",
    "    \n",
    "    return img,faceImg,rawImg\n",
    "\n",
    "def savePlot(inDict):\n",
    "    colors = ['red', 'green', 'orange', 'gray', 'blue', 'magenta']\n",
    "    plt.close('all')\n",
    "    imFiles = inDict['imFiles']\n",
    "    faceCascade = inDict['faceCascade']\n",
    "    emotionLabels = inDict['emotionLabels']\n",
    "    pca = inDict['pca']\n",
    "    clf = inDict['clf']\n",
    "    saveName = inDict['saveName']\n",
    "    allProbs = []\n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(9,3 * len(imFiles)),frameon=False);\n",
    "    gs = gridspec.GridSpec(len(imFiles), 2, width_ratios=[1,2]) \n",
    "    ct = -1\n",
    "    for i in range(len(imFiles)):\n",
    "        # Load the image\n",
    "        findDict = dict([\n",
    "            ('imFile',imFiles[i]),\n",
    "            ('scaleSize',1.05),\n",
    "            ('numNeighbors',3),\n",
    "            ('faceSize',50),\n",
    "            ('outImgSize',500),\n",
    "            ('faceCascade',faceCascade),\n",
    "        ])\n",
    "        rawImg,faces = findFace(findDict)\n",
    "\n",
    "\n",
    "        # Pull out image for classification\n",
    "        faceDict = dict([\n",
    "            ('faces',faces),\n",
    "            ('rawImg',rawImg),\n",
    "            ('outImgSize',500),\n",
    "            ('showImage',0),\n",
    "        ])\n",
    "        img,faceImg,rawImg = face2vec(faceDict)\n",
    "        \n",
    "        if len(faces):\n",
    "            # Classify image\n",
    "            testFace = np.asarray(img)\n",
    "            X_test_pca = pca(testFace)\n",
    "            probs = clf(X_test_pca)\n",
    "        \n",
    "        else:\n",
    "            probs = [0] * len(colors)\n",
    "            probs[0] = [0] * len(colors)\n",
    "            \n",
    "        allProbs.append(probs[0])\n",
    "\n",
    "        # Face image\n",
    "        ct = ct + 1\n",
    "        ax0 = plt.subplot(gs[ct])\n",
    "        plt.imshow(cv2.cvtColor(rawImg, cv2.COLOR_BGR2RGB))\n",
    "        plt.tick_params(\n",
    "            axis='both', \n",
    "            left='off', \n",
    "            top='off', \n",
    "            right='off', \n",
    "            bottom='off', \n",
    "            labelleft='off', \n",
    "            labeltop='off', \n",
    "            labelright='off', \n",
    "            labelbottom='off')\n",
    "\n",
    "        # Probability plot\n",
    "        ct = ct + 1\n",
    "        width = 1\n",
    "        ax1 = plt.subplot(gs[ct])\n",
    "        index = np.arange(len(emotionLabels))\n",
    "        ax2 = plt.bar(index * width,probs[0],width)\n",
    "        ax2[0].set_color(colors[0])\n",
    "        ax2[1].set_color(colors[1])\n",
    "        ax2[2].set_color(colors[2])\n",
    "        ax2[3].set_color(colors[3])\n",
    "        ax2[4].set_color(colors[4])\n",
    "        ax2[5].set_color(colors[5])\n",
    "        ax1.set_ylim([0, 1])\n",
    "        plt.xticks(index+0.5, emotionLabels,fontsize=12)\n",
    "        plt.ylabel('Probability',fontsize=12)\n",
    "             \n",
    "    plt.savefig(saveName)\n",
    "    return allProbs\n",
    "    \n",
    "def saveMovieFrames(inDict):\n",
    "    inMovie = inDict['inMovie']\n",
    "    saveDir = inDict['saveDir']\n",
    "    frameNames = []\n",
    "    if not os.path.isdir(saveDir):\n",
    "        os.makedirs(saveDir)\n",
    "    \n",
    "    cap = cv2.VideoCapture(inMovie)\n",
    "    success,image = cap.read()\n",
    "    count = 0\n",
    "    while success:\n",
    "        success,image = cap.read()\n",
    "        if success:\n",
    "            cv2.imwrite(saveDir + \"frame%d.jpg\" % count, image)     # save frame as JPEG file\n",
    "            frameNames.append(saveDir + \"frame%d.jpg\" % count)\n",
    "            count += 1\n",
    "    cap.release()\n",
    "    return frameNames\n",
    "\n",
    "def saveOutPlot(inDict):\n",
    "    colors = ['red', 'green', 'orange', 'gray', 'blue', 'magenta']\n",
    "    plt.close('all')\n",
    "    allProbs = np.asarray(inDict['allProbs'])\n",
    "    emotionLabels = inDict['emotionLabels']\n",
    "    outName = inDict['outName']\n",
    "    \n",
    "    # Convert to percent signal change\n",
    "    psc = ((allProbs / np.mean(allProbs,0)) - 1) * 100\n",
    "    psc = movingAverage(psc,5)\n",
    "    \n",
    "    fig = plt.figure(figsize=(12,6),frameon=False);\n",
    "    for i in range(psc.shape[1]):\n",
    "        plt.plot(psc[:,i],colors[i],linewidth=5.0);\n",
    "        \n",
    "    ax = plt.gca()\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    \n",
    "    # interpolate x for shading\n",
    "    x = range(psc.shape[0])\n",
    "    x2 = np.linspace(x[0], x[-1], len(x) * 100)\n",
    "\n",
    "    ct = 0\n",
    "    psc2 = []\n",
    "    for i in psc.T:\n",
    "        psc2.append(np.interp(x2, x, psc[:, ct]))\n",
    "        ct = ct + 1\n",
    "\n",
    "    psc2 = np.transpose(np.asarray(psc2))\n",
    "    maxPSC = psc2.max(1)\n",
    "\n",
    "    ax.fill_between(x2, ymin, ymax, where=psc2[:, 0] == maxPSC, alpha=0.33, facecolor=colors[0])\n",
    "    ax.fill_between(x2, ymin, ymax, where=psc2[:, 1] == maxPSC, alpha=0.33, facecolor=colors[1])\n",
    "    ax.fill_between(x2, ymin, ymax, where=psc2[:, 2] == maxPSC, alpha=0.33, facecolor=colors[2])\n",
    "    ax.fill_between(x2, ymin, ymax, where=psc2[:, 3] == maxPSC, alpha=0.33, facecolor=colors[3])\n",
    "    ax.fill_between(x2, ymin, ymax, where=psc2[:, 4] == maxPSC, alpha=0.33, facecolor=colors[4])\n",
    "    ax.fill_between(x2, ymin, ymax, where=psc2[:, 5] == maxPSC, alpha=0.33, facecolor=colors[5])\n",
    "\n",
    "    # Adjust legend, axes\n",
    "    plt.legend(emotionLabels)\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim([0, psc.shape[0] - 1])\n",
    "\n",
    "    plt.xlabel('Frame Number',fontsize=12)\n",
    "    plt.ylabel('Percent change',fontsize=12)\n",
    "    # save plot\n",
    "    plt.savefig(outName)\n",
    "    \n",
    "def movingAverage(data, window_size):\n",
    "    y = []\n",
    "    window = np.ones(int(window_size))/float(window_size)\n",
    "    for x in data.T:\n",
    "        tmp = np.convolve(x, window, 'full')\n",
    "        y.append(tmp[0:len(data)])\n",
    "    return np.transpose(np.asarray(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set variable names and defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set home direcory\n",
    "home = expanduser(\"~\")\n",
    "\n",
    "# Save file\n",
    "saveDir = home + '/InsightProject/websiteFiles/'\n",
    "saveFile = home + '/InsightProject/websiteFiles/save.p'\n",
    "saveMovie = home + '/InsightProject/websiteFiles/faceMovie.mp4'\n",
    "\n",
    "emotions = ['neutral','anger','contempt','disgust','fear','happy','sadness','surprise']\n",
    "emotionLabels = ['anger','disgust','happy','neutral','sadness','surprise']\n",
    "\n",
    "# OpenCV face classifier\n",
    "cascPath = home + '/anaconda2/pkgs/opencv-3.1.0-np111py27_1/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml'\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "# Face processing defaults\n",
    "outImgSize = 500\n",
    "scaleSize = 1.05\n",
    "numNeighbors = 3\n",
    "faceSize = 150\n",
    "showImage = 0\n",
    "\n",
    "# Classifier\n",
    "n_components = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load images, detect faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set paths, variable values\n",
    "dbDir = home + '/InsightProject/faceDatabases/CK+/'\n",
    "emoDir = home + '/InsightProject/faceDatabases/CK+/Emotion/'\n",
    "emoNums = [3,1,6,0,5,7]\n",
    "\n",
    "print(\"Loading input images...\")\n",
    "t0 = time()\n",
    "\n",
    "# Initialize variables\n",
    "subjectNames = []\n",
    "subjectFiles = []\n",
    "subjectEmotions = []\n",
    "subjectImages = []\n",
    "\n",
    "for subDir in glob.glob(emoDir + 'S*/'):\n",
    "    subject = os.path.basename(os.path.dirname(subDir))\n",
    "    \n",
    "    for faceDir in glob.glob(subDir + '*/'):\n",
    "        dirNum = os.path.basename(os.path.dirname(faceDir))\n",
    "        for thisFile in glob.glob(faceDir + '*.txt'):\n",
    "            imDir = os.path.join(dbDir,subject,dirNum)\n",
    "            fName = os.path.basename(thisFile[:-12])\n",
    "            imFile = os.path.join(imDir,fName + '.png')\n",
    "            \n",
    "            # Get the emotion\n",
    "            with open(thisFile) as f:\n",
    "                for line in f:\n",
    "                    line = map(float, line.split())\n",
    "                    lineInt = int(line[0])\n",
    "                    # ignore 'contempt' and 'fear'\n",
    "                    if lineInt != 2 and lineInt !=4:\n",
    "                        thisEmotion = emotions[lineInt]\n",
    "            \n",
    "            # Find the face\n",
    "            inDict = dict([\n",
    "                ('imFile',imFile),\n",
    "                ('scaleSize',scaleSize),\n",
    "                ('numNeighbors',numNeighbors),\n",
    "                ('faceSize',faceSize),\n",
    "                ('outImgSize',outImgSize),\n",
    "                ('faceCascade',faceCascade),\n",
    "            ])\n",
    "            rawImg,faces = findFace(inDict)\n",
    "\n",
    "            # If a face is detected\n",
    "            if len(faces): \n",
    "                inDict = dict([\n",
    "                    ('faces',faces),\n",
    "                    ('rawImg',rawImg),\n",
    "                    ('outImgSize',outImgSize),\n",
    "                    ('showImage',showImage),\n",
    "                    ])\n",
    "                img = face2vec(inDict)\n",
    "                \n",
    "                # Append to subjectNames\n",
    "                subjectNames.append(subject)\n",
    "                # Append to subjectEmotions\n",
    "                subjectEmotions.append(thisEmotion)\n",
    "                # Append to subjectFiles\n",
    "                subjectFiles.append(imFile) \n",
    "                # Append to output matrices\n",
    "                subjectImages.append(img[0]) \n",
    "                \n",
    "#             # Find the flipped face\n",
    "#             inDict = dict([\n",
    "#                 ('imFile',imFile),\n",
    "#                 ('scaleSize',scaleSize),\n",
    "#                 ('numNeighbors',numNeighbors),\n",
    "#                 ('faceSize',faceSize),\n",
    "#                 ('outImgSize',outImgSize),\n",
    "#                 ('reshape',reshape),\n",
    "#                 ('faceCascade',faceCascade),\n",
    "#             ])\n",
    "#             rawImg,faces = findFlippedFace(inDict)\n",
    "\n",
    "#             # If a face is detected\n",
    "#             if len(faces): \n",
    "#                 inDict = dict([\n",
    "#                     ('faces',faces),\n",
    "#                     ('rawImg',rawImg),\n",
    "#                     ('outImgSize',outImgSize),\n",
    "#                     ('showImage',showImage),\n",
    "#                     ])\n",
    "#                 img = face2vec(inDict)\n",
    "                \n",
    "#                 # Append to subjectNames\n",
    "#                 subjectNames.append(subject)\n",
    "#                 # Append to subjectEmotions\n",
    "#                 subjectEmotions.append(thisEmotion)\n",
    "#                 # Append to subjectFiles\n",
    "#                 subjectFiles.append(imFile) \n",
    "#                 # Append to output matrices\n",
    "#                 subjectImages.append(img[0])\n",
    "                \n",
    "            # Load the neutral image\n",
    "            imFile = os.path.join(imDir,fName[:-3] + '001.png')\n",
    "            \n",
    "            # Find the face\n",
    "            inDict = dict([\n",
    "                ('imFile',imFile),\n",
    "                ('scaleSize',scaleSize),\n",
    "                ('numNeighbors',numNeighbors),\n",
    "                ('faceSize',faceSize),\n",
    "                ('outImgSize',outImgSize),\n",
    "                ('faceCascade',faceCascade),\n",
    "            ])\n",
    "            rawImg,faces = findFace(inDict)\n",
    "\n",
    "            # If a face is detected\n",
    "            if len(faces):\n",
    "                inDict = dict([\n",
    "                    ('faces',faces),\n",
    "                    ('rawImg',rawImg),\n",
    "                    ('outImgSize',outImgSize),\n",
    "                    ('showImage',showImage),\n",
    "                    ])\n",
    "                img = face2vec(inDict)\n",
    "                \n",
    "                # Append to subjectNames\n",
    "                subjectNames.append(subject)\n",
    "                # Append to subjectEmotions\n",
    "                subjectEmotions.append('neutral')\n",
    "                # Append to subjectFiles\n",
    "                subjectFiles.append(imFile) \n",
    "                # Append to output matrices\n",
    "                subjectImages.append(img[0]) \n",
    "                \n",
    "# Convert to numpy array    \n",
    "subjectNames = np.asarray(subjectNames)\n",
    "subjectEmotions = np.asarray(subjectEmotions)\n",
    "subjectFiles = np.asarray(subjectFiles)\n",
    "subjectImages = np.squeeze(np.asarray(subjectImages))\n",
    "\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "for emotion in emotions:\n",
    "    print(emotion)\n",
    "    emotionSummary = np.array(subjectEmotions == emotion)\n",
    "    print(np.sum(emotionSummary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = subjectImages\n",
    "Y_train = subjectEmotions\n",
    "\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\"\n",
    "      % (n_components, X_train.shape[0]))\n",
    "t0 = time()\n",
    "pca = PCA(n_components, svd_solver='randomized',\n",
    "          whiten=True).fit(X_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
    "t0 = time()\n",
    "X_train_pca = pca.transform(X_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "print(\"Fitting the classifier to the training set\")\n",
    "t0 = time()\n",
    "\n",
    "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1],\n",
    "             'kernel': ['rbf']}\n",
    "\n",
    "clf = GridSearchCV(SVC(class_weight='balanced',probability=True), param_grid)\n",
    "\n",
    "clf = clf.fit(X_train_pca, Y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Best estimator found by grid search:\")\n",
    "\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save PCA and model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Saving the training set to file\")\n",
    "t0 = time()\n",
    "pcaTransform = pca.transform\n",
    "clfPredictProba = clf.predict_proba\n",
    "dill.dump([pcaTransform,clfPredictProba],open(saveFile,'w'))\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the PCA spectrum\n",
    "print(\"Finding the PCA variance explained\")\n",
    "t0 = time()\n",
    "pca.fit(subjectImages)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "ax = plt.figure(1, figsize=(4, 3))\n",
    "plt.clf()\n",
    "plt.plot(pca.explained_variance_ratio_, linewidth=2)\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('proportion explained variance')\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([0, 100])\n",
    "plt.axvline(n_components,linestyle=':', label='n_components chosen')\n",
    "plt.legend(prop=dict(size=12))\n",
    "plt.show()\n",
    "print('Variance explained by first 50 components:')\n",
    "print(sum(pca.explained_variance_ratio_[0:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    subjectImages, subjectEmotions, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\"\n",
    "      % (n_components, X_train.shape[0]))\n",
    "t0 = time()\n",
    "pca = PCA(n_components, svd_solver='randomized',\n",
    "          whiten=True).fit(X_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
    "t0 = time()\n",
    "X_train_pca = pca.transform(X_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "print(\"Fitting the classifier to the training set\")\n",
    "t0 = time()\n",
    "clf = SVC(class_weight='balanced',probability=True,C=1000,gamma=0.01,kernel='rbf')\n",
    "clf = clf.fit(X_train_pca, Y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "# Plot results\n",
    "X_test_pca = pca.transform(X_test)\n",
    "Y_pred = clf.predict(X_test_pca)\n",
    "probs = clf.predict_proba(X_test_pca)\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "\n",
    "print(\"Accuracy score:\")\n",
    "print(accuracy_score(Y_test, Y_pred))\n",
    "print(\"Precision score:\")\n",
    "print(precision_score(Y_test, Y_pred,average='weighted'))\n",
    "print(\"Recall score:\")\n",
    "print(recall_score(Y_test, Y_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf_arr = confusion_matrix(Y_test, Y_pred)\n",
    "norm_conf = []\n",
    "for i in conf_arr:\n",
    "    a = 0\n",
    "    tmp_arr = []\n",
    "    a = sum(i, 0)\n",
    "    for j in i:\n",
    "        tmp_arr.append(float(j)/float(a))\n",
    "    norm_conf.append(tmp_arr)\n",
    "\n",
    "fig = plt.figure(1,figsize=(8, 8))\n",
    "\n",
    "plt.clf()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_aspect(1)\n",
    "res = ax.imshow(np.array(norm_conf), cmap=plt.cm.coolwarm, \n",
    "                interpolation='nearest')\n",
    "\n",
    "width, height = conf_arr.shape\n",
    "\n",
    "for x in xrange(width):\n",
    "    for y in xrange(height):\n",
    "        ax.annotate(str(conf_arr[x][y]), xy=(y, x), \n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center')\n",
    "\n",
    "plt.xticks(range(width), emotionLabels[:width])\n",
    "plt.yticks(range(height), emotionLabels[:height])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Novel image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Loading the training set to file\")\n",
    "t0 = time()\n",
    "pca, clf = pickle.load(open(saveFile,'rb'))\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save figures\n",
    "saveName = '/Users/abock/InsightProject/novelImage.png'\n",
    "imFiles = ['/Users/abock/InsightProject/imageFiles/01-sad-Keanu.png',\n",
    "           '/Users/abock/InsightProject/imageFiles/02-disgust-Picard.png',\n",
    "           '/Users/abock/Downloads/puzzle.jpg',\n",
    "           '/Users/abock/InsightProject/imageFiles/03-anger-Kid.png',\n",
    "           '/Users/abock/InsightProject/imageFiles/04-happy-Ysela.png',\n",
    "          ]\n",
    "inDict = dict([\n",
    "    ('imFiles',imFiles),\n",
    "    ('faceCascade',faceCascade),\n",
    "    ('emotionLabels',emotionLabels),\n",
    "    ('faceCascade',faceCascade),\n",
    "    ('pca',pca),\n",
    "    ('clf',clf),\n",
    "    ('saveName',saveName),\n",
    "])\n",
    "allProbs = savePlot(inDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imFiles = ['/Users/abock/InsightProject/imageFiles/01-sad-Keanu.png',\n",
    "           '/Users/abock/InsightProject/imageFiles/02-disgust-Picard.png',\n",
    "           '/Users/abock/Desktop/foo.png',\n",
    "           '/Users/abock/InsightProject/imageFiles/03-anger-Kid.png',\n",
    "           '/Users/abock/InsightProject/imageFiles/04-happy-Ysela.png',\n",
    "          ]\n",
    "findDict = dict([\n",
    "    ('imFile',imFiles[2]),\n",
    "    ('scaleSize',1.05),\n",
    "    ('numNeighbors',3),\n",
    "    ('faceSize',50),\n",
    "    ('outImgSize',500),\n",
    "    ('faceCascade',faceCascade),\n",
    "])\n",
    "rawImg,faces = findFace(findDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pull out image for classification\n",
    "faceDict = dict([\n",
    "    ('faces',faces),\n",
    "    ('rawImg',rawImg),\n",
    "    ('outImgSize',500),\n",
    "    ('showImage',0),\n",
    "])\n",
    "img,faceImg,rawImg = face2vec(faceDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save movie output plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get frame names\n",
    "inDict = dict([\n",
    "    ('inMovie','/Users/abock/InsightProject/websiteFiles/subject001/faceMovie.mp4'),\n",
    "    ('saveDir','/Users/abock/InsightProject/websiteFiles/subject001/faceMovie/'),\n",
    "])\n",
    "frameNames = saveMovieFrames(inDict)\n",
    "\n",
    "# Get probabilities, save output frames\n",
    "saveName = '/Users/abock/InsightProject/websiteFiles/subject001/outFrames.jpg'\n",
    "inDict = dict([\n",
    "    ('imFiles',frameNames),\n",
    "    ('faceCascade',faceCascade),\n",
    "    ('emotionLabels',emotionLabels),\n",
    "    ('saveName',saveName),\n",
    "    ('pca',pca),\n",
    "    ('clf',clf),\n",
    "])\n",
    "allProbs = savePlot(inDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save output plot\n",
    "outName = '/Users/abock/InsightProject/websiteFiles/subject001/outPlot.jpg'\n",
    "inDict = dict([\n",
    "    ('allProbs',allProbs),\n",
    "    ('emotionLabels',emotionLabels),\n",
    "    ('outName',outName),\n",
    "])\n",
    "saveOutPlot(inDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imFile = '/Users/abock/Desktop/InsightProject/imageFiles/angry-guy.jpg'\n",
    "imFile = '/Users/abock/InsightProject/imageFiles/01-sad-Keanu.png'\n",
    "#imFile = '/Users/abock/Desktop/InsightProject/imageFiles/happy-guy.png'\n",
    "#imFile = '/Users/abock/Desktop/InsightProject/imageFiles/sadDawson.jpg'\n",
    "\n",
    "#imFile = '/Users/abock/Desktop/InsightProject/websiteFiles/andrewBock/images/frame1.jpg'\n",
    "\n",
    "#imFile = '/Users/abock/Downloads/winonaRyder/frame590.jpg'\n",
    "\n",
    "#imFile = '/Users/abock/Desktop/InsightProject/imageFiles/aia_sadness_547.png'\n",
    "#imFile = '/Users/abock/Desktop/InsightProject/imageFiles/aia_anger_93.png'\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "cascPath = '/usr/local/Cellar/opencv/2.4.13.2/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml'\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "# Load the image\n",
    "inDict = dict([\n",
    "    ('imFile',imFile),\n",
    "    ('scaleSize',1.05),\n",
    "    ('numNeighbors',3),\n",
    "    ('faceSize',10),\n",
    "    ('outImgSize',500),\n",
    "    ('faceCascade',faceCascade),\n",
    "])\n",
    "rawImg,faces = findFace(inDict)\n",
    "\n",
    "# Pull out image for classification\n",
    "inDict = dict([\n",
    "    ('faces',faces),\n",
    "    ('rawImg',rawImg),\n",
    "    ('outImgSize',outImgSize),\n",
    "    ('showImage',0),\n",
    "])\n",
    "img,faceImg,rawImg = face2vec(inDict)\n",
    "\n",
    "# Classify image\n",
    "testFace = np.asarray(img)\n",
    "X_test_pca = pca(testFace)\n",
    "probs = clf(X_test_pca)\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(9,3));\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1,2]);\n",
    "\n",
    "# Face image \n",
    "ax0 = plt.subplot(gs[0])\n",
    "plt.imshow(cv2.cvtColor(rawImg, cv2.COLOR_BGR2RGB))\n",
    "#plt.xlabel(Y_pred,fontsize=12)\n",
    "plt.tick_params(\n",
    "    axis='both', \n",
    "    left='off', \n",
    "    top='off', \n",
    "    right='off', \n",
    "    bottom='off', \n",
    "    labelleft='off', \n",
    "    labeltop='off', \n",
    "    labelright='off', \n",
    "    labelbottom='off')\n",
    "\n",
    "# Probability plot\n",
    "width = 1\n",
    "ax1 = plt.subplot(gs[1])\n",
    "index = np.arange(len(emotionLabels))\n",
    "ax2 = plt.bar(index * width,probs[0],width)\n",
    "ax2[0].set_color('r')\n",
    "ax2[1].set_color('g')\n",
    "ax2[2].set_color('y')\n",
    "ax2[3].set_color('c')\n",
    "ax2[4].set_color('b')\n",
    "ax2[5].set_color('k')\n",
    "ax1.set_ylim([0, 1])\n",
    "plt.xticks(index+0.5, emotionLabels,fontsize=12)\n",
    "plt.ylabel('Probability',fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save frames AND plot as movie - andrewBock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inDir = '/Users/abock/Desktop/InsightProject/websiteFiles/andrewBock/images/'\n",
    "outDir = '/Users/abock/Desktop/InsightProject/websiteFiles/andrewBock/figures/'\n",
    "\n",
    "for i in range(0,99):\n",
    "    imFile = (inDir + \"frame%d.jpg\" % i)\n",
    "    saveName = (outDir + \"frame%d.jpg\" % i)\n",
    "    \n",
    "    # Load the image\n",
    "    inDict = dict([\n",
    "        ('imFile',imFile),\n",
    "        ('scaleSize',1.05),\n",
    "        ('numNeighbors',3),\n",
    "        ('faceSize',10),\n",
    "        ('outImgSize',outImgSize),\n",
    "        ('faceCascade',faceCascade),\n",
    "    ])\n",
    "\n",
    "    rawImg,faces = findFace(inDict)\n",
    "\n",
    "    # Pull out image for classification\n",
    "    inDict = dict([\n",
    "        ('faces',faces),\n",
    "        ('rawImg',rawImg),\n",
    "        ('outImgSize',outImgSize),\n",
    "        ('showImage',0),\n",
    "    ])\n",
    "    img,faceImg,rawImg = face2vec(inDict)\n",
    "    \n",
    "    \n",
    "    testFace = np.asarray(img)\n",
    "    X_test_pca = pca.transform(testFace)\n",
    "    probs = clf.predict_proba(X_test_pca)\n",
    "        \n",
    "    inDict = dict([\n",
    "        ('imFiles', imFiles),\n",
    "        ('faceCascade', faceCascade),\n",
    "        ('emotionLabels', emotionLabels),\n",
    "        ('pca', pca),\n",
    "        ('clf',clf),\n",
    "        ('saveName', plotName),\n",
    "        ])\n",
    "    savePlot(inDict)\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record Movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "\n",
    "outDir = '/Users/abock/Desktop/faceMovie/'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "cap = cv2.VideoCapture(0)\n",
    "w = cap.get(cv2.CAP_PROP_FRAME_WIDTH);\n",
    "h = cap.get(cv2.CAP_PROP_FRAME_HEIGHT); \n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(saveMovie,fourcc, 15.0, (int(w),int(h)))\n",
    "\n",
    "time.sleep(5) # wait for camera to catch up\n",
    "ct = 0\n",
    "while ct < 100:\n",
    "    ct = ct + 1\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret==True:\n",
    "        #frame = cv2.flip(frame,0)\n",
    "\n",
    "        # write the flipped frame\n",
    "        out.write(frame)\n",
    "\n",
    "        cv2.imshow('frame',frame)\n",
    "        cv2.waitKey(30)\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save frames as movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileNames = [\n",
    "    '/Users/abock/Desktop/InsightProject/websiteFiles/frame10.jpg',\n",
    "    '/Users/abock/Desktop/InsightProject/websiteFiles/frame11.jpg',\n",
    "    '/Users/abock/Desktop/InsightProject/websiteFiles/frame12.jpg',\n",
    "    '/Users/abock/Desktop/InsightProject/websiteFiles/frame13.jpg',\n",
    "    '/Users/abock/Desktop/InsightProject/websiteFiles/frame14.jpg',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "cap = cv2.VideoCapture(0)\n",
    "w = cap.get(cv2.CAP_PROP_FRAME_WIDTH);\n",
    "h = cap.get(cv2.CAP_PROP_FRAME_HEIGHT); \n",
    "cap.release()\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('foo.mp4',fourcc, 15.0, (int(w),int(h)))\n",
    "for fileName in fileNames:\n",
    "    foo = cv2.imread(fileName)\n",
    "    out.write(foo)\n",
    "    cv2.imshow('foo',foo)\n",
    "    cv2.waitKey(30)\n",
    "\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    subjectImages, subjectEmotions, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "svm = SVC(class_weight='balanced',probability=True);\n",
    "# Make the 'pipe'\n",
    "svmPipe = Pipeline(steps=[('pca', pca), ('svm', svm)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the PCA spectrum\n",
    "print(\"Plotting the PCA\")\n",
    "t0 = time()\n",
    "pca.fit(subjectImages)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "ax = plt.figure(1, figsize=(4, 3))\n",
    "plt.clf()\n",
    "plt.plot(pca.explained_variance_ratio_, linewidth=2)\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('explained_variance_ratio_')\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Finding the best SVM parameters\")\n",
    "t0 = time()\n",
    "n_components = [50,75,100]\n",
    "#n_components = 50\n",
    "C = [1e3, 5e3, 1e4, 5e4, 1e5]\n",
    "gamma = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "\n",
    "#Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "\n",
    "clfSVM = GridSearchCV(svmPipe,\n",
    "                         dict(pca__n_components=n_components,\n",
    "                              svm__C=C,svm__gamma=gamma))\n",
    "\n",
    "clfSVM.fit(subjectImages, subjectEmotions)\n",
    "\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clfSVM.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    subjectImages, subjectEmotions, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_pred = clfSVM.predict(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save out the pca and classifier\n",
    "print(\"Saving the training set to file\")\n",
    "t0 = time()\n",
    "pickle.dump([pca,clf],open(saveFile,'w'))\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2 \n",
    "import urllib\n",
    "import os\n",
    "\n",
    "def saveURLimages(url,outDir):\n",
    "    req = urllib2.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    html = urllib2.urlopen(req).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    # get images\n",
    "    images = [img for img in soup.findAll('img')]\n",
    "    print (str(len(images)) + \" images found.\")\n",
    "    print 'Downloading images to: ' + outDir\n",
    "    \n",
    "    # download images\n",
    "    image_links = [each.get('src') for each in images]\n",
    "    #print(image_links)\n",
    "    for each in image_links:\n",
    "        if not each is None:\n",
    "            if each[0:4] == 'http':\n",
    "                #print(each)\n",
    "                filename=each.split('/')[-1]\n",
    "                urllib.urlretrieve(each,os.path.join(outDir,filename + '.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save happy faces (Google Images)\n",
    "happyPerson = 'https://www.google.com/search?site=&tbm=isch&source=hp&biw=1680&bih=949&q=happy+person&oq=happy+person&gs_l=img.3..0l10.15311.17067.0.17213.12.7.0.5.5.0.82.450.7.7.0....0...1ac.1.64.img..0.12.461.FpA8-IvoCOk'\n",
    "url = happyPerson\n",
    "outDir = '/Users/abock/Desktop/HappyFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "saveURLimages(url,outDir)\n",
    "\n",
    "# Save sad faces (Google Images)\n",
    "sadPerson = 'https://www.google.com/search?site=&tbm=isch&source=hp&biw=840&bih=949&q=sad+person&oq=sad+person&gs_l=img.3..0l10.732.2268.0.2413.10.7.0.3.3.0.337.1010.0j1j2j1.4.0....0...1ac.1.64.img..3.7.1015.rkrdqlI1p-k'\n",
    "url = sadPerson\n",
    "outDir = '/Users/abock/Desktop/SadFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "saveURLimages(url,outDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save happy faces (Bing Images)\n",
    "happyPerson = 'https://www.bing.com/images/search?q=happy%20person&qs=n&form=QBIR&pq=happy%20person&sc=8-12&sp=-1&sk='\n",
    "url = happyPerson\n",
    "outDir = '/Users/abock/Desktop/HappyFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "saveURLimages(url,outDir)\n",
    "\n",
    "# Save sad faces (Bing Images)\n",
    "sadPerson = 'https://www.bing.com/images/search?q=sad+person&qs=n&form=QBILPG&pq=sad+person&sc=8-8&sp=-1&sk='\n",
    "url = sadPerson\n",
    "outDir = '/Users/abock/Desktop/SadFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "saveURLimages(url,outDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Happy Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save happy faces (Yahoo Images)\n",
    "outDir = '/Users/abock/Desktop/HappyFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "    \n",
    "happyPerson = 'https://images.search.yahoo.com/search/images;_ylt=AwrTcXdqJYFYvE0A0juLuLkF?p=happy+person&ei=UTF-8&iscqry=&fr=sfp'\n",
    "url = happyPerson\n",
    "saveURLimages(url,outDir)\n",
    "\n",
    "happyHumanFace = 'https://images.search.yahoo.com/search/images;_ylt=AwrTcXyAJoFYWHMApB6JzbkF;_ylu=X3oDMTBsZ29xY3ZzBHNlYwNzZWFyY2gEc2xrA2J1dHRvbg--;_ylc=X1MDOTYwNjI4NTcEX3IDMgRhY3RuA2NsawRiY2sDNmltaTE4bGM2N29ybyUyNmIlM0QzJTI2cyUzRDI5BGNzcmNwdmlkAzNzeFdhVEl3Tmk1cFdrRkZXR1BqZUFkSE5qVXVPQUFBQUFDdHdqNncEZnIDc2ZwBGZyMgNzYS1ncARncHJpZANLQm5lbEt4X1JOeUlzNXY5WC40cVRBBG10ZXN0aWQDbnVsbARuX3N1Z2cDMTAEb3JpZ2luA2ltYWdlcy5zZWFyY2gueWFob28uY29tBHBvcwMwBHBxc3RyAwRwcXN0cmwDBHFzdHJsAzE2BHF1ZXJ5A2hhcHB5IGh1bWFuIGZhY2UEdF9zdG1wAzE0ODQ4NTkwNzYEdnRlc3RpZANudWxs?gprid=KBnelKx_RNyIs5v9X.4qTA&pvid=3sxWaTIwNi5pWkFFWGPjeAdHNjUuOAAAAACtwj6w&p=happy+human+face&fr=sfp&fr2=sb-top-images.search.yahoo.com&ei=UTF-8&n=60&x=wrt'\n",
    "url = happyHumanFace\n",
    "saveURLimages(url,outDir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sad Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output directory\n",
    "outDir = '/Users/abock/Desktop/SadFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "   \n",
    "# Yahoo Images\n",
    "sadPerson = 'https://images.search.yahoo.com/search/images;_ylt=AwrTcXZ6JYFYl68AvSGJzbkF;_ylu=X3oDMTBsZ29xY3ZzBHNlYwNzZWFyY2gEc2xrA2J1dHRvbg--;_ylc=X1MDOTYwNjI4NTcEX3IDMgRhY3RuA2NsawRiY2sDNmltaTE4bGM2N29ybyUyNmIlM0QzJTI2cyUzRDI5BGNzcmNwdmlkA3VBdHlfekl3Tmk1cFdrRkZXR1BqZUFDaE5qVXVPQUFBQUFDZUhiR3oEZnIDc2ZwBGZyMgNzYS1ncARncHJpZANnb3NOVG13V1RTdVc1MXA5ZmhMZGlBBG10ZXN0aWQDbnVsbARuX3N1Z2cDMTAEb3JpZ2luA2ltYWdlcy5zZWFyY2gueWFob28uY29tBHBvcwMwBHBxc3RyAwRwcXN0cmwDBHFzdHJsAzEwBHF1ZXJ5A3NhZCBwZXJzb24EdF9zdG1wAzE0ODQ4NTg3NjAEdnRlc3RpZANudWxs?gprid=gosNTmwWTSuW51p9fhLdiA&pvid=uAty_zIwNi5pWkFFWGPjeAChNjUuOAAAAACeHbGz&p=sad+person&fr=sfp&fr2=sb-top-images.search.yahoo.com&ei=UTF-8&n=60&x=wrt'\n",
    "url = sadPerson\n",
    "saveURLimages(url,outDir)\n",
    "\n",
    "sadHumanFace = 'https://images.search.yahoo.com/search/images;_ylt=AwrTcXmHJYFY_lwACdOJzbkF;_ylu=X3oDMTBsZ29xY3ZzBHNlYwNzZWFyY2gEc2xrA2J1dHRvbg--;_ylc=X1MDOTYwNjI4NTcEX3IDMgRhY3RuA2NsawRiY2sDNmltaTE4bGM2N29ybyUyNmIlM0QzJTI2cyUzRDI5BGNzcmNwdmlkA2N3Q2RJREl3Tmk1cFdrRkZXR1BqZUFUVU5qVXVPQUFBQUFDZTZzVXEEZnIDc2ZwBGZyMgNzYS1ncARncHJpZANKZHp4ZFBPMVFfSzFlYlZTTkpycGhBBG10ZXN0aWQDbnVsbARuX3N1Z2cDMTAEb3JpZ2luA2ltYWdlcy5zZWFyY2gueWFob28uY29tBHBvcwMwBHBxc3RyAwRwcXN0cmwDBHFzdHJsAzE0BHF1ZXJ5A3NhZCBodW1hbiBmYWNlBHRfc3RtcAMxNDg0ODU5MDA5BHZ0ZXN0aWQDbnVsbA--?gprid=JdzxdPO1Q_K1ebVSNJrphA&pvid=cwCdIDIwNi5pWkFFWGPjeATUNjUuOAAAAACe6sUq&p=sad+human+face&fr=sfp&fr2=sb-top-images.search.yahoo.com&ei=UTF-8&n=60&x=wrt'\n",
    "url = sadHumanFace\n",
    "saveURLimages(url,outDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Set paths, variable values\n",
    "# dbDir = '/Users/abock/Desktop/InsightProject/faceDatabases/FERG/'\n",
    "\n",
    "# print(\"Loading input images...\")\n",
    "# t0 = time()\n",
    "\n",
    "# # Find the images and emotion labels\n",
    "# for subDir in glob.glob(dbDir + '*/'):\n",
    "#     subject = os.path.basename(os.path.dirname(subDir))\n",
    "    \n",
    "#     for faceDir in glob.glob(subDir + '*/'):\n",
    "#         tmp = os.path.basename(os.path.dirname(faceDir))\n",
    "#         thisEmotion = tmp[len(subject)+1:]\n",
    "#         for imFile in glob.glob(faceDir + '*.png'):\n",
    "#             # Load the image\n",
    "#             rawImg = cv2.imread(imFile)\n",
    "\n",
    "#             # Detect faces in the image\n",
    "#             faces = faceCascade.detectMultiScale(\n",
    "#             rawImg,\n",
    "#             scaleFactor=scaleSize,\n",
    "#             minNeighbors=numNeighbors,\n",
    "#             minSize=(faceSize, faceSize),\n",
    "#             flags = cv2.CASCADE_SCALE_IMAGE\n",
    "#             )\n",
    "\n",
    "#             # If a face is detected\n",
    "#             if len(faces):\n",
    "#                 # Convert to gray\n",
    "#                 gImg = cv2.cvtColor(rawImg, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#                 # Resize just the face\n",
    "#                 x,y,w,h = faces[0,:]\n",
    "#                 faceImg = gImg[y:y+h,x:x+w]\n",
    "#                 img = cv2.resize(faceImg,(outImgSize,outImgSize),interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "# #                 cv2.imshow('Image',img)\n",
    "# #                 cv2.waitKey(30)\n",
    "\n",
    "#                 # Flatten\n",
    "#                 l = img.shape[0] * img.shape[1]\n",
    "#                 img = img.reshape(1, l)\n",
    "                \n",
    "#                 # Append to subjectEmotions\n",
    "#                 subjectNames.append(subject)\n",
    "#                 # Append to subjectEmotions\n",
    "#                 subjectEmotions.append(thisEmotion)\n",
    "#                 # Append to subjectFiles\n",
    "#                 subjectFiles.append(imFile) \n",
    "#                 # Append to output matrices\n",
    "#                 subjectImages.append(img[0])\n",
    "                \n",
    "# print(\"done in %0.3fs\" % (time() - t0))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
