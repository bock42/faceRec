{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# faceRec\n",
    "\n",
    "This IPython notebook will classify the emotion of a face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cohn-Kanade AU-Coded Expression Database - CK+\n",
    " \n",
    "123 subjects (85 female)\n",
    "\n",
    "emotional expressions were scored/validated, so there are a variable number of emotions / subject \n",
    "\n",
    "happy, sad, surprise, neutral, anger, disgust, fear \n",
    "\n",
    "\n",
    "CK+ Notes:\n",
    "\n",
    "The Emotion coded files (Emotion_labels.zip) - ONLY 327 of the 593 sequences have emotion sequences. This is because these are the only ones the fit the prototypic definition. Like the FACS files, there is only 1 Emotion file for each sequence which is the last frame (the peak frame). There should be only one entry and the number will range from 0-7 (i.e. 0=neutral, 1=anger, 2=contempt, 3=disgust, 4=fear, 5=happy, 6=sadness, 7=surprise). N.B there is only 327 files- IF THERE IS NO FILE IT MEANS THAT THERE IS NO EMOTION LABEL (sorry to be explicit but this will avoid confusion)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the faces and emotion files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from time import time\n",
    "import cPickle as pickle\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import neighbors\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Save file\n",
    "saveDir = '/Users/abock/Desktop/InsightProject/websiteFiles/'\n",
    "saveFile = '/Users/abock/Desktop/InsightProject/websiteFiles/save.p'\n",
    "saveMovie = '/Users/abock/Desktop/InsightProject/websiteFiles/faceMovie.mp4'\n",
    "\n",
    "# Initialize variables\n",
    "subjectNames = []\n",
    "subjectFiles = []\n",
    "subjectEmotions = []\n",
    "subjectImages = []\n",
    "\n",
    "emotions = ['neutral','anger','contempt','disgust','fear','happy','sadness','surprise']\n",
    "emotionLabels = ['anger','disgust','happy','neutral','sadness','surprise']\n",
    "\n",
    "# OpenCV face classifier\n",
    "cascPath = '/usr/local/Cellar/opencv/2.4.13.2/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml'\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "# Face processing defaults\n",
    "outImgSize = 500\n",
    "scaleSize = 1.05\n",
    "numNeighbors = 3\n",
    "faceSize = 150\n",
    "\n",
    "# Classifier\n",
    "n_components = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CK+ images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input images...\n",
      "done in 39.538s\n"
     ]
    }
   ],
   "source": [
    "# Set paths, variable values\n",
    "dbDir = '/Users/abock/Desktop/InsightProject/faceDatabases/CK+/'\n",
    "emoDir = '/Users/abock/Desktop/InsightProject/faceDatabases/CK+/Emotion/'\n",
    "# emoNums = [1,3,4,5,0,6,7]\n",
    "emoNums = [3,1,6,0,5,7]\n",
    "\n",
    "print(\"Loading input images...\")\n",
    "t0 = time()\n",
    "\n",
    "# Find the images and emotion labels\n",
    "ct = 0;\n",
    "thisEmotion = []\n",
    "for subDir in glob.glob(emoDir + 'S*/'):\n",
    "    subject = os.path.basename(os.path.dirname(subDir))\n",
    "    # Append to subjectNames\n",
    "    subjectNames.append(subject)\n",
    "    for faceDir in glob.glob(subDir + '*/'):\n",
    "        dirNum = os.path.basename(os.path.dirname(faceDir))\n",
    "        for thisFile in glob.glob(faceDir + '*.txt'):\n",
    "            imDir = os.path.join(dbDir,subject,dirNum)\n",
    "            fName = os.path.basename(thisFile[:-12])\n",
    "            imFile = os.path.join(imDir,fName + '.png')\n",
    "            \n",
    "            with open(thisFile) as f:\n",
    "                for line in f:\n",
    "                    line = map(float, line.split())\n",
    "                    lineInt = int(line[0])\n",
    "#                     if lineInt != 2: # 2 is contempt\n",
    "                    if lineInt != 2 and lineInt !=4: # 2 is contempt, 4 is fear\n",
    "                        thisEmotion = emotions[lineInt]\n",
    "\n",
    "            # Load the emotion image\n",
    "            rawImg = cv2.imread(imFile)\n",
    "\n",
    "            # Detect faces in the image\n",
    "            faces = faceCascade.detectMultiScale(\n",
    "            rawImg,\n",
    "            scaleFactor=scaleSize,\n",
    "            minNeighbors=numNeighbors,\n",
    "            minSize=(faceSize, faceSize),\n",
    "            flags = cv2.CASCADE_SCALE_IMAGE\n",
    "            )\n",
    "\n",
    "            # If a face is detected\n",
    "            if len(faces):\n",
    "                # Convert to gray\n",
    "                gImg = cv2.cvtColor(rawImg, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # Resize just the face\n",
    "                x,y,w,h = faces[0,:]\n",
    "                faceImg = gImg[y:y+h,x:x+w]\n",
    "                img = cv2.resize(faceImg,(outImgSize,outImgSize),interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "#                 cv2.imshow('Image',img)\n",
    "#                 cv2.waitKey(30)\n",
    "\n",
    "                # Flatten\n",
    "                l = img.shape[0] * img.shape[1]\n",
    "                img = img.reshape(1, l)\n",
    "                \n",
    "                # Append to subjectNames\n",
    "                subjectNames.append(subject)\n",
    "                # Append to subjectEmotions\n",
    "                subjectEmotions.append(thisEmotion)\n",
    "                # Append to subjectFiles\n",
    "                subjectFiles.append(imFile) \n",
    "                # Append to output matrices\n",
    "                subjectImages.append(img[0]) \n",
    "                \n",
    "            # Load the neutral image\n",
    "            imFile = os.path.join(imDir,fName[:-3] + '001.png')\n",
    "            rawImg = cv2.imread(imFile)\n",
    "\n",
    "            # Detect faces in the image\n",
    "            faces = faceCascade.detectMultiScale(\n",
    "            rawImg,\n",
    "            scaleFactor=scaleSize,\n",
    "            minNeighbors=numNeighbors,\n",
    "            minSize=(faceSize, faceSize),\n",
    "            flags = cv2.CASCADE_SCALE_IMAGE\n",
    "            )\n",
    "\n",
    "            # If a face is detected\n",
    "            if len(faces):\n",
    "                # Convert to gray\n",
    "                gImg = cv2.cvtColor(rawImg, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # Resize just the face\n",
    "                x,y,w,h = faces[0,:]\n",
    "                faceImg = gImg[y:y+h,x:x+w]\n",
    "                img = cv2.resize(faceImg,(outImgSize,outImgSize),interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "#                 cv2.imshow('Image',img)\n",
    "#                 cv2.waitKey(30)\n",
    "\n",
    "                # Flatten\n",
    "                l = img.shape[0] * img.shape[1]\n",
    "                img = img.reshape(1, l)\n",
    "                \n",
    "                # Append to subjectNames\n",
    "                subjectNames.append(subject)\n",
    "                # Append to subjectEmotions\n",
    "                subjectEmotions.append('neutral')\n",
    "                # Append to subjectFiles\n",
    "                subjectFiles.append(imFile) \n",
    "                # Append to output matrices\n",
    "                subjectImages.append(img[0]) \n",
    "                \n",
    "# Convert to numpy array    \n",
    "subjectNames = np.asarray(subjectNames)\n",
    "subjectEmotions = np.asarray(subjectEmotions)\n",
    "subjectFiles = np.asarray(subjectFiles)\n",
    "subjectImages = np.asarray(subjectImages)\n",
    "\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the top 50 eigenfaces from 654 faces\n",
      "done in 15.473s\n",
      "Projecting the input data on the eigenfaces orthonormal basis\n",
      "done in 1.349s\n",
      "Fitting the classifier to the training set\n",
      "done in 11.847s\n",
      "Best estimator found by grid search:\n",
      "SVC(C=1000.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "X_train = subjectImages\n",
    "Y_train = subjectEmotions\n",
    "\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\"\n",
    "      % (n_components, X_train.shape[0]))\n",
    "t0 = time()\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized',\n",
    "          whiten=True).fit(X_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
    "t0 = time()\n",
    "X_train_pca = pca.transform(X_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "print(\"Fitting the classifier to the training set\")\n",
    "t0 = time()\n",
    "\n",
    "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "\n",
    "clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced',probability=True), param_grid)\n",
    "#clf = GridSearchCV(SVC(kernel='rbf',probability=True), param_grid)\n",
    "\n",
    "clf = clf.fit(X_train_pca, Y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral\n",
      "327\n",
      "anger\n",
      "56\n",
      "contempt\n",
      "0\n",
      "disgust\n",
      "62\n",
      "fear\n",
      "0\n",
      "happy\n",
      "73\n",
      "sadness\n",
      "45\n",
      "surprise\n",
      "91\n"
     ]
    }
   ],
   "source": [
    "for emotion in emotions:\n",
    "    print(emotion)\n",
    "    emotionSummary = np.array(subjectEmotions == emotion)\n",
    "    print(np.sum(emotionSummary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the training set to file\n",
      "done in 7.086s\n"
     ]
    }
   ],
   "source": [
    "# Save out the pca and classifier\n",
    "print(\"Saving the training set to file\")\n",
    "t0 = time()\n",
    "pickle.dump([pca,clf],open(saveFile,'w'))\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the top 50 eigenfaces from 523 faces\n",
      "done in 14.433s\n",
      "Projecting the input data on the eigenfaces orthonormal basis\n",
      "done in 1.371s\n",
      "Fitting the classifier to the training set\n",
      "done in 8.872s\n",
      "Best estimator found by grid search:\n",
      "SVC(C=1000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    subjectImages, subjectEmotions, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\"\n",
    "      % (n_components, X_train.shape[0]))\n",
    "t0 = time()\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized',\n",
    "          whiten=True).fit(X_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
    "t0 = time()\n",
    "X_train_pca = pca.transform(X_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "print(\"Fitting the classifier to the training set\")\n",
    "t0 = time()\n",
    "\n",
    "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "\n",
    "#clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced',probability=True), param_grid)\n",
    "clf = GridSearchCV(SVC(kernel='rbf',probability=True), param_grid)\n",
    "\n",
    "# metrics = ['minkowski','euclidean','manhattan'] \n",
    "# weights = ['uniform','distance'] #10.0**np.arange(-5,4)\n",
    "# NN = np.arange(1,30)\n",
    "# #algorithms = ['ball_tree', 'kd_tree', 'brute']\n",
    "# algorithms = ['brute']\n",
    "# param_grid = dict(metric=metrics,weights=weights,n_neighbors=NN,algorithm=algorithms)\n",
    "# clf = GridSearchCV(neighbors.KNeighborsClassifier(), param_grid)\n",
    "\n",
    "clf = clf.fit(X_train_pca, Y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      anger       0.55      0.30      0.39        20\n",
      "    disgust       0.67      0.75      0.71         8\n",
      "      happy       0.93      0.81      0.87        16\n",
      "    neutral       0.79      0.89      0.84        65\n",
      "    sadness       0.40      0.57      0.47         7\n",
      "   surprise       0.79      0.73      0.76        15\n",
      "\n",
      "avg / total       0.74      0.75      0.74       131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test_pca = pca.transform(X_test)\n",
    "Y_pred = clf.predict(X_test_pca)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Novel image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the training set to file\n",
      "done in 32.117s\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading the training set to file\")\n",
    "t0 = time()\n",
    "pca, clf = pickle.load(open(saveFile,'rb'))\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0c83ec9598d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0minFace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/abock/Desktop/InsightProject/websiteFiles/frame10.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0moutImgSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "#inFace = '/Users/abock/Desktop/InsightProject/sad_keanu.png'\n",
    "#inFace = '/Users/abock/Desktop/InsightProject/crazy-guy.png'\n",
    "inFace = '/Users/abock/Desktop/InsightProject/happy_guy.png'\n",
    "#inFace = '/Users/abock/Desktop/InsightProject/happy_girl.png'\n",
    "#inFace = '/Users/abock/Desktop/InsightProject/smiling-women.jpg'\n",
    "#inFace = '/Users/abock/Desktop/InsightProject/sadGoslingMeme.jpg'\n",
    "#inFace = '/Users/abock/Desktop/InsightProject/sadDawson.jpg'\n",
    "\n",
    "#inFace = '/Users/abock/Desktop/InsightProject/aia_sadness_547.png'\n",
    "#inFace = '/Users/abock/Desktop/InsightProject/aia_anger_93.png'\n",
    "#inFace = '/Users/abock/Desktop/InsightProject/malcolm_joy_104.png'\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "outImgSize = 500\n",
    "scaleSize = 1.05\n",
    "numNeighbors = 3\n",
    "faceSize = 10\n",
    "\n",
    "# Load the image\n",
    "rawImg = cv2.imread(inFace)\n",
    "img = rawImg\n",
    "img = cv2.resize(rawImg, (outImgSize, outImgSize), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "# Detect faces in the image\n",
    "faces = faceCascade.detectMultiScale(\n",
    "    img,\n",
    "    scaleFactor=scaleSize,\n",
    "    minNeighbors=1,\n",
    "    minSize=(faceSize, faceSize),\n",
    "    flags=cv2.CASCADE_SCALE_IMAGE\n",
    ")\n",
    "\n",
    "# Get the largest face\n",
    "i = np.where(faces[:, 2] == faces[:, 2].max())\n",
    "faces = faces[i, :]\n",
    "faces = faces[0, :]\n",
    "x, y, w, h = faces[0, :]\n",
    "\n",
    "# Rectangle on face\n",
    "faceImage = img\n",
    "cv2.rectangle(faceImage, (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
    "cv2.imshow('Face',faceImage)\n",
    "cv2.waitKey(30)\n",
    "\n",
    "# Resize just the face\n",
    "croppedImage = img[y:y + h, x:x + w]\n",
    "\n",
    "# Classify input image\n",
    "if len(faces):\n",
    "    # Convert to gray\n",
    "    gImg = cv2.cvtColor(croppedImage, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Resize just the face\n",
    "    img = cv2.resize(gImg, (outImgSize, outImgSize), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    # Flatten\n",
    "    l = img.shape[0] * img.shape[1]\n",
    "    img = img.reshape(1, l)\n",
    "\n",
    "    testFace = np.asarray(img)\n",
    "    X_test_pca = pca.transform(testFace)\n",
    "    Y_pred = clf.predict(X_test_pca)\n",
    "    print(Y_pred)\n",
    "\n",
    "    # Classification figure\n",
    "    probs = clf.predict_proba(X_test_pca)\n",
    "    df = DataFrame(probs, columns=emotionLabels)\n",
    "    ax = df.plot.bar()\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_ylabel('Probability')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record Movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "\n",
    "outDir = '/Users/abock/Desktop/faceMovie/'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "cap = cv2.VideoCapture(0)\n",
    "w = cap.get(cv2.CAP_PROP_FRAME_WIDTH);\n",
    "h = cap.get(cv2.CAP_PROP_FRAME_HEIGHT); \n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(saveMovie,fourcc, 15.0, (int(w),int(h)))\n",
    "\n",
    "time.sleep(5) # wait for camera to catch up\n",
    "ct = 0\n",
    "while ct < 100:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret==True:\n",
    "        #frame = cv2.flip(frame,0)\n",
    "\n",
    "        # write the flipped frame\n",
    "        out.write(frame)\n",
    "\n",
    "        cv2.imshow('frame',frame)\n",
    "        cv2.waitKey(30)\n",
    "\n",
    "    ct = ct + 1\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save movie as frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(saveMovie)\n",
    "success,image = cap.read()\n",
    "count = 0\n",
    "success = True\n",
    "while success:\n",
    "  success,image = cap.read()\n",
    "  cv2.imwrite(saveDir + \"frame%d.jpg\" % count, image)     # save frame as JPEG file\n",
    "  count += 1\n",
    "    \n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2 \n",
    "import urllib\n",
    "import os\n",
    "\n",
    "def saveURLimages(url,outDir):\n",
    "    req = urllib2.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    html = urllib2.urlopen(req).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    # get images\n",
    "    images = [img for img in soup.findAll('img')]\n",
    "    print (str(len(images)) + \" images found.\")\n",
    "    print 'Downloading images to: ' + outDir\n",
    "    \n",
    "    # download images\n",
    "    image_links = [each.get('src') for each in images]\n",
    "    #print(image_links)\n",
    "    for each in image_links:\n",
    "        if not each is None:\n",
    "            if each[0:4] == 'http':\n",
    "                #print(each)\n",
    "                filename=each.split('/')[-1]\n",
    "                urllib.urlretrieve(each,os.path.join(outDir,filename + '.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save happy faces (Google Images)\n",
    "happyPerson = 'https://www.google.com/search?site=&tbm=isch&source=hp&biw=1680&bih=949&q=happy+person&oq=happy+person&gs_l=img.3..0l10.15311.17067.0.17213.12.7.0.5.5.0.82.450.7.7.0....0...1ac.1.64.img..0.12.461.FpA8-IvoCOk'\n",
    "url = happyPerson\n",
    "outDir = '/Users/abock/Desktop/HappyFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "saveURLimages(url,outDir)\n",
    "\n",
    "# Save sad faces (Google Images)\n",
    "sadPerson = 'https://www.google.com/search?site=&tbm=isch&source=hp&biw=840&bih=949&q=sad+person&oq=sad+person&gs_l=img.3..0l10.732.2268.0.2413.10.7.0.3.3.0.337.1010.0j1j2j1.4.0....0...1ac.1.64.img..3.7.1015.rkrdqlI1p-k'\n",
    "url = sadPerson\n",
    "outDir = '/Users/abock/Desktop/SadFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "saveURLimages(url,outDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save happy faces (Bing Images)\n",
    "happyPerson = 'https://www.bing.com/images/search?q=happy%20person&qs=n&form=QBIR&pq=happy%20person&sc=8-12&sp=-1&sk='\n",
    "url = happyPerson\n",
    "outDir = '/Users/abock/Desktop/HappyFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "saveURLimages(url,outDir)\n",
    "\n",
    "# Save sad faces (Bing Images)\n",
    "sadPerson = 'https://www.bing.com/images/search?q=sad+person&qs=n&form=QBILPG&pq=sad+person&sc=8-8&sp=-1&sk='\n",
    "url = sadPerson\n",
    "outDir = '/Users/abock/Desktop/SadFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "saveURLimages(url,outDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Happy Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save happy faces (Yahoo Images)\n",
    "outDir = '/Users/abock/Desktop/HappyFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "    \n",
    "happyPerson = 'https://images.search.yahoo.com/search/images;_ylt=AwrTcXdqJYFYvE0A0juLuLkF?p=happy+person&ei=UTF-8&iscqry=&fr=sfp'\n",
    "url = happyPerson\n",
    "saveURLimages(url,outDir)\n",
    "\n",
    "happyHumanFace = 'https://images.search.yahoo.com/search/images;_ylt=AwrTcXyAJoFYWHMApB6JzbkF;_ylu=X3oDMTBsZ29xY3ZzBHNlYwNzZWFyY2gEc2xrA2J1dHRvbg--;_ylc=X1MDOTYwNjI4NTcEX3IDMgRhY3RuA2NsawRiY2sDNmltaTE4bGM2N29ybyUyNmIlM0QzJTI2cyUzRDI5BGNzcmNwdmlkAzNzeFdhVEl3Tmk1cFdrRkZXR1BqZUFkSE5qVXVPQUFBQUFDdHdqNncEZnIDc2ZwBGZyMgNzYS1ncARncHJpZANLQm5lbEt4X1JOeUlzNXY5WC40cVRBBG10ZXN0aWQDbnVsbARuX3N1Z2cDMTAEb3JpZ2luA2ltYWdlcy5zZWFyY2gueWFob28uY29tBHBvcwMwBHBxc3RyAwRwcXN0cmwDBHFzdHJsAzE2BHF1ZXJ5A2hhcHB5IGh1bWFuIGZhY2UEdF9zdG1wAzE0ODQ4NTkwNzYEdnRlc3RpZANudWxs?gprid=KBnelKx_RNyIs5v9X.4qTA&pvid=3sxWaTIwNi5pWkFFWGPjeAdHNjUuOAAAAACtwj6w&p=happy+human+face&fr=sfp&fr2=sb-top-images.search.yahoo.com&ei=UTF-8&n=60&x=wrt'\n",
    "url = happyHumanFace\n",
    "saveURLimages(url,outDir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sad Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output directory\n",
    "outDir = '/Users/abock/Desktop/SadFaces'\n",
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "   \n",
    "# Yahoo Images\n",
    "sadPerson = 'https://images.search.yahoo.com/search/images;_ylt=AwrTcXZ6JYFYl68AvSGJzbkF;_ylu=X3oDMTBsZ29xY3ZzBHNlYwNzZWFyY2gEc2xrA2J1dHRvbg--;_ylc=X1MDOTYwNjI4NTcEX3IDMgRhY3RuA2NsawRiY2sDNmltaTE4bGM2N29ybyUyNmIlM0QzJTI2cyUzRDI5BGNzcmNwdmlkA3VBdHlfekl3Tmk1cFdrRkZXR1BqZUFDaE5qVXVPQUFBQUFDZUhiR3oEZnIDc2ZwBGZyMgNzYS1ncARncHJpZANnb3NOVG13V1RTdVc1MXA5ZmhMZGlBBG10ZXN0aWQDbnVsbARuX3N1Z2cDMTAEb3JpZ2luA2ltYWdlcy5zZWFyY2gueWFob28uY29tBHBvcwMwBHBxc3RyAwRwcXN0cmwDBHFzdHJsAzEwBHF1ZXJ5A3NhZCBwZXJzb24EdF9zdG1wAzE0ODQ4NTg3NjAEdnRlc3RpZANudWxs?gprid=gosNTmwWTSuW51p9fhLdiA&pvid=uAty_zIwNi5pWkFFWGPjeAChNjUuOAAAAACeHbGz&p=sad+person&fr=sfp&fr2=sb-top-images.search.yahoo.com&ei=UTF-8&n=60&x=wrt'\n",
    "url = sadPerson\n",
    "saveURLimages(url,outDir)\n",
    "\n",
    "sadHumanFace = 'https://images.search.yahoo.com/search/images;_ylt=AwrTcXmHJYFY_lwACdOJzbkF;_ylu=X3oDMTBsZ29xY3ZzBHNlYwNzZWFyY2gEc2xrA2J1dHRvbg--;_ylc=X1MDOTYwNjI4NTcEX3IDMgRhY3RuA2NsawRiY2sDNmltaTE4bGM2N29ybyUyNmIlM0QzJTI2cyUzRDI5BGNzcmNwdmlkA2N3Q2RJREl3Tmk1cFdrRkZXR1BqZUFUVU5qVXVPQUFBQUFDZTZzVXEEZnIDc2ZwBGZyMgNzYS1ncARncHJpZANKZHp4ZFBPMVFfSzFlYlZTTkpycGhBBG10ZXN0aWQDbnVsbARuX3N1Z2cDMTAEb3JpZ2luA2ltYWdlcy5zZWFyY2gueWFob28uY29tBHBvcwMwBHBxc3RyAwRwcXN0cmwDBHFzdHJsAzE0BHF1ZXJ5A3NhZCBodW1hbiBmYWNlBHRfc3RtcAMxNDg0ODU5MDA5BHZ0ZXN0aWQDbnVsbA--?gprid=JdzxdPO1Q_K1ebVSNJrphA&pvid=cwCdIDIwNi5pWkFFWGPjeATUNjUuOAAAAACe6sUq&p=sad+human+face&fr=sfp&fr2=sb-top-images.search.yahoo.com&ei=UTF-8&n=60&x=wrt'\n",
    "url = sadHumanFace\n",
    "saveURLimages(url,outDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Set paths, variable values\n",
    "# dbDir = '/Users/abock/Desktop/InsightProject/faceDatabases/FERG/'\n",
    "\n",
    "# print(\"Loading input images...\")\n",
    "# t0 = time()\n",
    "\n",
    "# # Find the images and emotion labels\n",
    "# for subDir in glob.glob(dbDir + '*/'):\n",
    "#     subject = os.path.basename(os.path.dirname(subDir))\n",
    "    \n",
    "#     for faceDir in glob.glob(subDir + '*/'):\n",
    "#         tmp = os.path.basename(os.path.dirname(faceDir))\n",
    "#         thisEmotion = tmp[len(subject)+1:]\n",
    "#         for imFile in glob.glob(faceDir + '*.png'):\n",
    "#             # Load the image\n",
    "#             rawImg = cv2.imread(imFile)\n",
    "\n",
    "#             # Detect faces in the image\n",
    "#             faces = faceCascade.detectMultiScale(\n",
    "#             rawImg,\n",
    "#             scaleFactor=scaleSize,\n",
    "#             minNeighbors=numNeighbors,\n",
    "#             minSize=(faceSize, faceSize),\n",
    "#             flags = cv2.CASCADE_SCALE_IMAGE\n",
    "#             )\n",
    "\n",
    "#             # If a face is detected\n",
    "#             if len(faces):\n",
    "#                 # Convert to gray\n",
    "#                 gImg = cv2.cvtColor(rawImg, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#                 # Resize just the face\n",
    "#                 x,y,w,h = faces[0,:]\n",
    "#                 faceImg = gImg[y:y+h,x:x+w]\n",
    "#                 img = cv2.resize(faceImg,(outImgSize,outImgSize),interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "# #                 cv2.imshow('Image',img)\n",
    "# #                 cv2.waitKey(30)\n",
    "\n",
    "#                 # Flatten\n",
    "#                 l = img.shape[0] * img.shape[1]\n",
    "#                 img = img.reshape(1, l)\n",
    "                \n",
    "#                 # Append to subjectEmotions\n",
    "#                 subjectNames.append(subject)\n",
    "#                 # Append to subjectEmotions\n",
    "#                 subjectEmotions.append(thisEmotion)\n",
    "#                 # Append to subjectFiles\n",
    "#                 subjectFiles.append(imFile) \n",
    "#                 # Append to output matrices\n",
    "#                 subjectImages.append(img[0])\n",
    "                \n",
    "# print(\"done in %0.3fs\" % (time() - t0))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
